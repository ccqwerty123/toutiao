import asyncio
import json
import random
import time
import math
import sys
from datetime import datetime
from pathlib import Path

from playwright.async_api import async_playwright, Page, BrowserContext

# ================= ä¾èµ–åº“æ£€æµ‹ =================
try:
    # å°è¯•å¯¼å…¥ playwright-stealth å¢å¼ºé˜²çˆ¬èƒ½åŠ›
    from playwright_stealth import stealth_async
    HAS_STEALTH = True
except ImportError:
    HAS_STEALTH = False
    print("================================================================")
    print(f"[WARN] æœªå®‰è£… playwright-stealth åº“ã€‚")
    print(f"[WARN] å»ºè®®è¿è¡Œ: pip install playwright-stealth ä»¥é™ä½è¢«æ£€æµ‹é£é™©ã€‚")
    print("================================================================")

# ================= é…ç½®åŒºåŸŸ =================

# ç›®æ ‡ç”¨æˆ·ä¸»é¡µ Token URL (è¯·ç¡®ä¿æ­¤é“¾æ¥æœ‰æ•ˆ)
TOUTIAO_URL = "https://www.toutiao.com/c/user/token/CiyRLPHkUyTCD9FmHodOGQVcmZh5-NRKyfiTSF0XMms-tSja0FdhrUWRp-T-DBpJCjwAAAAAAAAAAAAAT8lExjCbDHcWTgszQQjqU0Ohh9qtuXbuEOe6CQdqJEZ7yIpoM-NJ93_Sty1iMpOe_FUQ9ZmDDhjDxYPqBCIBA9GPpzc="

# è¾“å‡ºè®¾ç½®
DATA_DIR = Path("data")
DATA_DIR.mkdir(parents=True, exist_ok=True)

DB_FILE = DATA_DIR / "toutiao_db.json"
DEBUG_DIR = DATA_DIR / "debug"
DEBUG_DIR.mkdir(parents=True, exist_ok=True)

# æµè§ˆè¡Œä¸ºé™åˆ¶
MAX_READ_COUNT = 10     # æ¯æ¬¡è¿è¡Œè„šæœ¬æœ€å¤šé˜…è¯»å¤šå°‘ç¯‡
MIN_READ_COUNT = 3      # æ¯æ¬¡è¿è¡Œè„šæœ¬æœ€å°‘é˜…è¯»å¤šå°‘ç¯‡
MAX_SYNC_SCROLLS = 20   # åŒæ­¥åˆ—è¡¨æ—¶æœ€å¤§ä¸‹æ»‘æ¬¡æ•°
AGING_THRESHOLD = 50    # æ–‡ç« â€œè€åŒ–â€é˜ˆå€¼
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°


# ================= User-Agent ç®¡ç† =================

# å†…ç½®å…œåº• PC UA åº“ (è¦†ç›–ä¸»æµæµè§ˆå™¨ä¸æ“ä½œç³»ç»Ÿ)
FALLBACK_PC_UAS = [
    # Windows 10/11 Chrome
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    # Windows Edge
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
    # Mac Chrome
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, Like Gecko) Chrome/119.0.0.0 Safari/537.36",
    # Mac Safari
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    # Linux Chrome
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

def get_pc_user_agent():
    """
    ä¼˜å…ˆä½¿ç”¨ real-useragent åº“è·å–éšæœº PC UAã€‚
    å¦‚æœè·å–å¤±è´¥æˆ–åº“æœªå®‰è£…ï¼Œä½¿ç”¨å†…ç½®åˆ—è¡¨å…œåº•ã€‚
    """
    ua = ""
    try:
        from real_useragent import UserAgent
        rua = UserAgent()
        ua = rua.desktop_useragent()
        # ç®€å•æ ¡éªŒè·å–çš„UAæ˜¯å¦åˆæ³•
        if not ua or len(ua) < 20:
            raise ValueError("UA too short")
    except Exception:
        ua = random.choice(FALLBACK_PC_UAS)
    
    return ua

# å¸¸è§ PC åˆ†è¾¨ç‡åº“ (é¿å…å•ä¸€æŒ‡çº¹)
VIEWPORTS = [
    {"width": 1920, "height": 1080},
    {"width": 1536, "height": 864},
    {"width": 1440, "height": 900},
    {"width": 1366, "height": 768},
    {"width": 1280, "height": 720},
]

# ================= JS æ³¨å…¥è„šæœ¬ (æ ¸å¿ƒé€»è¾‘ä¼˜åŒ–) =================
# å¢å¼ºç‰ˆé“¾æ¥æå–è„šæœ¬ï¼šå…¼å®¹æ€§ä¿®å¤(ç§»é™¤?.) + å¢å¼ºæ ‡é¢˜æå– + è¿‡æ»¤è¯æ›´æ–°
EXTRACT_LINKS_JS = r"""
() => {
  const anchors = Array.from(document.querySelectorAll("a[href]"));
  const origin = window.location.origin;
  const results = [];
  const seen = new Set();
  
  // 1. è·¯å¾„ç‰¹å¾åˆ¤æ–­
  const isArticle = (path) => {
    if (!path) return false;
    if (path.startsWith("/c/user/")) return false;
    if (path.startsWith("/search/")) return false;
    if (path.includes("toutiao_search")) return false;

    const lastPart = path.split("/").filter(Boolean).pop();
    if (!lastPart) return false;
    const digits = lastPart.replace(/\D/g, "").length;
    return digits > 5;
  };

  // 2. åŸºç¡€æ–‡æœ¬æå–
  const getText = (el) => {
    if (!el) return "";
    let txt = (el.innerText || "").trim();
    if (txt) return txt;
    
    txt = (el.getAttribute("aria-label") || "").trim();
    if (txt) return txt;
    
    txt = (el.getAttribute("title") || "").trim();
    if (txt) return txt;
    
    const img = el.querySelector("img");
    if (img) {
        txt = (img.getAttribute("alt") || "").trim();
    }
    return txt;
  };

  // 3. æˆªå–æ–‡æœ¬
  const truncateText = (text, maxLength = 50) => {
    if (!text || text.length <= maxLength) return text;
    return text.substring(0, maxLength) + "...";
  };

  // 4. è·å–å†…å®¹ç±»å‹
  const getContentType = (url) => {
    if (url.includes("/article/")) return "article";
    if (url.includes("/w/")) return "weitoutiao";
    if (url.includes("/video/")) return "video";
    return "unknown";
  };

  // 5. å¢å¼ºçš„æ ‡é¢˜æå–
  const extractTitle = (a, urlObj) => {
    let text = getText(a);
    const contentType = getContentType(urlObj.pathname);

    // å¦‚æœç›´æ¥è·å–å¤±è´¥æˆ–æ–‡æœ¬å¤ªçŸ­ï¼Œå°è¯•æ›´å¤šæ–¹æ³•
    if (!text || text.length < 4) {
        let container = a.closest('.feed-card-wrapper, .article-card, .feed-card-article-wrapper, .card-wrapper, .weitoutiao-wrap, .wtt-content');
        
        // --- å…¼å®¹æ€§ä¿®æ”¹ï¼šä¸ä½¿ç”¨ ?. æ“ä½œç¬¦ ---
        if (!container) {
            if (a.parentElement && a.parentElement.parentElement && a.parentElement.parentElement.parentElement) {
                container = a.parentElement.parentElement.parentElement;
            }
        }

        if (container) {
            // å¾®å¤´æ¡ç­–ç•¥
            if (contentType === "weitoutiao") {
                const contentEl = container.querySelector('.weitoutiao-content, .wtt-content, .feed-card-article-content, [class*="content"]');
                if (contentEl) {
                    const content = contentEl.innerText.trim();
                    if (content) {
                        text = truncateText(content, 40);
                    }
                }
            } 
            // è§†é¢‘ç­–ç•¥
            else if (contentType === "video") {
                const titleEl = container.querySelector('.video-title, .title, [class*="title"]');
                if (titleEl) {
                    const t = titleEl.innerText.trim();
                    if (t) text = t;
                }
            }

            // é€šç”¨æ ‡é¢˜æŸ¥æ‰¾
            if (!text || text.length < 4) {
                const selectors = [
                    '.title', '.feed-card-article-title', '.article-title', '.feed-card-article-l a',
                    '[class*="title"]', 'h1, h2, h3', '.text', 'p'
                ];

                for (const selector of selectors) {
                    const el = container.querySelector(selector);
                    if (el) {
                        const t = el.innerText.trim();
                        if (t && t.length > 4) {
                            text = truncateText(t, 50);
                            break;
                        }
                    }
                }
            }

            // æœ€åå°è¯•ï¼šè·å–å®¹å™¨å†…ç¬¬ä¸€ä¸ªé•¿æ–‡æœ¬
            if (!text || text.length < 4) {
                const allTexts = container.innerText.trim().split('\n').filter(t => t.trim().length > 4);
                if (allTexts.length > 0) {
                    text = truncateText(allTexts[0], 50);
                }
            }
        }
    }

    // å…œåº•é‡å‘½å
    if (!text || text === "Untitled") {
        if (contentType === "weitoutiao") text = "[å¾®å¤´æ¡]";
        else if (contentType === "video") text = "[è§†é¢‘]";
    }

    return { text: text || "Untitled", contentType };
  };

  // ä¸»å¾ªç¯
  for (const a of anchors) {
    let href = a.getAttribute("href");
    if (!href) continue;
    
    if (href.startsWith("/")) href = origin + href;
    
    try {
        const urlObj = new URL(href);
        
        if (!urlObj.hostname.includes("toutiao.com")) continue;
        if (!isArticle(urlObj.pathname)) continue;
        
        const cleanUrl = urlObj.origin + urlObj.pathname;
        if (seen.has(cleanUrl)) continue;

        const titleInfo = extractTitle(a, urlObj);
        let text = titleInfo.text;

        // å…³é”®è¯è¿‡æ»¤ (å·²æ·»åŠ 'ä¾µæƒä¸¾æŠ¥å—ç†å…¬ç¤º')
        const filterKeywords = [
            'è·Ÿå¸–è¯„è®ºè‡ªå¾‹ç®¡ç†æ‰¿è¯ºä¹¦',
            'ç”¨æˆ·åè®®',
            'éšç§æ”¿ç­–',
            'ä¾µæƒæŠ•è¯‰',
            'ç½‘ç»œè°£è¨€æ›å…‰å°',
            'è¿æ³•å’Œä¸è‰¯ä¿¡æ¯ä¸¾æŠ¥',
            'ä¾µæƒä¸¾æŠ¥å—ç†å…¬ç¤º'
        ];
        
        if (filterKeywords.some(keyword => text.includes(keyword))) continue;

        // é¢å¤–çš„çŸ­è¯è¿‡æ»¤
        if (!text.startsWith('[') && text.match(/^(å¤‡æ¡ˆ|ä¸¾æŠ¥|ç™»å½•|ä¸‹è½½|å¹¿å‘Š|ç›¸å…³æ¨è|æœç´¢)$/)) continue;
        
        seen.add(cleanUrl);
        results.push({ 
            text: text, 
            href: cleanUrl,
            type: titleInfo.contentType
        });

    } catch(e) {}
  }

  return results;
}
"""

# ================= æ•°æ®åº“ç®¡ç†ç±» =================

class ArticleDB:
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.data = self._load()

    def _load(self):
        if not self.db_path.exists():
            return {"last_sync_date": "", "articles": {}}
        try:
            return json.loads(self.db_path.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[DB] è¯»å–æ•°æ®åº“å‡ºé”™: {e}ï¼Œå°†åˆå§‹åŒ–æ–°åº“")
            return {"last_sync_date": "", "articles": {}}

    def save(self):
        try:
            self.db_path.write_text(json.dumps(self.data, indent=2, ensure_ascii=False), encoding="utf-8")
        except Exception as e:
            print(f"[DB] ä¿å­˜å¤±è´¥: {e}")

    def needs_sync(self) -> bool:
        """åˆ¤æ–­ä»Šå¤©æ˜¯å¦å·²ç»æ‰§è¡Œè¿‡å…¨é‡æŠ“å–"""
        today = datetime.now().strftime("%Y-%m-%d")
        return self.data.get("last_sync_date") != today

    def mark_synced(self):
        self.data["last_sync_date"] = datetime.now().strftime("%Y-%m-%d")
        self.save()

    def add_articles(self, scraped_items: list):
        """å¢é‡æ·»åŠ æ–‡ç« """
        added_count = 0
        current_urls = self.data["articles"]
        
        for item in scraped_items:
            url = item['href']
            # å¦‚æœæ˜¯æ–°é“¾æ¥ï¼Œæˆ–è€…æ—§é“¾æ¥æ˜¯Untitledä½†è¿™æ¬¡æŠ“åˆ°äº†çœŸæ ‡é¢˜ï¼Œåˆ™æ›´æ–°
            if url not in current_urls:
                current_urls[url] = {
                    "title": item['text'],
                    "url": url,
                    "status": "active",
                    "last_read_at": "",
                    "read_count": 0
                }
                added_count += 1
            elif current_urls[url]["title"] == "Untitled" and item['text'] != "Untitled":
                 current_urls[url]["title"] = item['text'] # ä¿®æ­£æ ‡é¢˜
        
        print(f"[DB] æ•°æ®åº“æ›´æ–°: æ–°å¢ {added_count} ç¯‡ï¼Œå½“å‰æ€»åº“å­˜ {len(current_urls)} ç¯‡")
        self.save()

    def mark_invalid(self, url):
        """æ ‡è®°å¤±æ•ˆ"""
        if url in self.data["articles"]:
            self.data["articles"][url]["status"] = "invalid"
            print(f"[DB] é“¾æ¥æ ‡è®°ä¸ºæ— æ•ˆ: {url}")
            self.save()

    def record_read(self, url):
        """è®°å½•é˜…è¯»"""
        if url in self.data["articles"]:
            today = datetime.now().strftime("%Y-%m-%d")
            entry = self.data["articles"][url]
            entry["last_read_at"] = today
            entry["read_count"] = entry.get("read_count", 0) + 1
            self.save()

    def get_weighted_candidates(self) -> list:
        """è·å–ä»Šæ—¥é˜…è¯»åˆ—è¡¨ï¼šæƒé‡ç®—æ³•"""
        today = datetime.now().strftime("%Y-%m-%d")
        candidates = []
        weights = []
        
        active_urls = [k for k, v in self.data["articles"].items() if v.get("status") == "active"]
        
        for url in active_urls:
            info = self.data["articles"][url]
            
            # è§„åˆ™1: ä»Šå¤©è¯»è¿‡çš„ç»å¯¹ä¸è¯»
            if info.get("last_read_at") == today:
                continue
            
            read_count = info.get("read_count", 0)
            
            # è§„åˆ™2: æƒé‡è®¡ç®—
            # æ²¡è¯»è¿‡çš„(0æ¬¡): æé«˜æƒé‡ 200
            # è¯»å¾—å°‘çš„(<5æ¬¡): é«˜æƒé‡ 100
            # æ™®é€š(<20æ¬¡): ä¸­æƒé‡ 50
            # è€æ—§(>50æ¬¡): ä½æƒé‡ 5 (ä¿ç•™å¾®å°æ¦‚ç‡)
            if read_count == 0:
                w = 200
            elif read_count < 5:
                w = 100
            elif read_count < 20:
                w = 50
            elif read_count < AGING_THRESHOLD:
                w = 20
            else:
                w = 5
            
            candidates.append(info)
            weights.append(w)
            
        if not candidates:
            return []

        # æ— æ”¾å›æŠ½å–
        target_k = random.randint(MIN_READ_COUNT, MAX_READ_COUNT)
        target_k = min(target_k, len(candidates))
        
        print(f"[PLAN] å¯é€‰æ–‡ç« åº“: {len(candidates)} ç¯‡. è®¡åˆ’é˜…è¯»: {target_k} ç¯‡")
        
        selected = []
        temp_cand = list(candidates)
        temp_weight = list(weights)
        
        for _ in range(target_k):
            if not temp_cand: break
            chosen = random.choices(temp_cand, weights=temp_weight, k=1)[0]
            selected.append(chosen)
            
            idx = temp_cand.index(chosen)
            temp_cand.pop(idx)
            temp_weight.pop(idx)
            
        return selected

# ================= æ‹ŸäººåŒ–æ“ä½œå‡½æ•° =================

async def human_delay(min_s=1.0, max_s=3.0):
    """å¸¦éšæœºæ€§çš„ç­‰å¾…"""
    await asyncio.sleep(random.uniform(min_s, max_s))

async def human_mouse_move(page: Page, x_target, y_target, steps=25):
    """è´å¡å°”æ›²çº¿æ¨¡æ‹Ÿé¼ æ ‡ç§»åŠ¨"""
    try:
        start_x = random.randint(100, 1000)
        start_y = random.randint(100, 600)
        
        ctrl_x1 = start_x + (x_target - start_x) * 0.3 + random.randint(-50, 50)
        ctrl_y1 = start_y + (y_target - start_y) * 0.3 + random.randint(-50, 50)
        ctrl_x2 = start_x + (x_target - start_x) * 0.7 + random.randint(-50, 50)
        ctrl_y2 = start_y + (y_target - start_y) * 0.7 + random.randint(-50, 50)

        for i in range(steps + 1):
            t = i / steps
            x = (1-t)**3 * start_x + 3*(1-t)**2 * t * ctrl_x1 + 3*(1-t)*t**2 * ctrl_x2 + t**3 * x_target
            y = (1-t)**3 * start_y + 3*(1-t)**2 * t * ctrl_y1 + 3*(1-t)*t**2 * ctrl_y2 + t**3 * y_target
            
            # æŠ–åŠ¨
            x += random.uniform(-2, 2)
            y += random.uniform(-2, 2)
            
            await page.mouse.move(x, y)
            await asyncio.sleep(random.uniform(0.005, 0.015))
    except Exception:
        pass

async def human_scroll(page: Page, max_scrolls=1):
    """æ‹ŸäººåŒ–æ»šåŠ¨"""
    for _ in range(max_scrolls):
        # éšæœºæ»šåŠ¨å¹…åº¦
        delta_y = random.randint(300, 700)
        await page.mouse.wheel(0, delta_y)
        
        # æ»šåŠ¨åçš„åœé¡¿ï¼Œæ¨¡æ‹Ÿé˜…è¯»
        await human_delay(1.0, 2.5)
        
        # 20% æ¦‚ç‡å›æ»š (å›çœ‹)
        if random.random() < 0.2:
            await page.mouse.wheel(0, -random.randint(100, 250))
            await human_delay(0.5, 1.2)

async def check_captcha(page: Page, tag="unknown") -> bool:
    """æ£€æŸ¥éªŒè¯ç ï¼Œå¹¶æˆªå›¾ï¼ˆè¦†ç›–æœ€æ–°ä¸€ä»½ï¼‰"""
    try:
        title = await page.title()
        is_captcha = False
        
        # 1. æ ‡é¢˜åˆ¤æ–­
        if any(kw in title for kw in ["éªŒè¯", "å®‰å…¨æ£€æµ‹", "captcha", "verify"]):
            is_captcha = True
            
        # 2. DOM åˆ¤æ–­
        if not is_captcha:
            if await page.query_selector("#captcha-verify-image") or \
               await page.query_selector(".captcha_verify_container"):
                is_captcha = True
        
        if is_captcha:
            print(f"[ALERT] {tag} é˜¶æ®µæ£€æµ‹åˆ°éªŒè¯ç ! Title: {title}")
            # ä¿å­˜éªŒè¯ç æˆªå›¾ï¼Œè¦†ç›–æ—§çš„åŒç±»å‹æ–‡ä»¶
            screenshot_path = DEBUG_DIR / f"captcha_{tag}_latest.png"
            await page.screenshot(path=screenshot_path)
            print(f"[ALERT] éªŒè¯ç æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
            return True
            
        return False
    except Exception as e:
        print(f"[WARN] éªŒè¯ç æ£€æµ‹å‡ºé”™: {e}")
        return False

# ================= æ ¸å¿ƒä»»åŠ¡é€»è¾‘ =================

async def sync_task(context: BrowserContext, db: ArticleDB):
    """
    å…¨é‡åŒæ­¥ä»»åŠ¡ - é­”æ”¹ç‰ˆ
    æ ¸å¿ƒä¿®æ”¹ï¼š
    1. ä¸çƒ­èº«ï¼ç›´æ¥è®¿é—®ç”¨æˆ·ä¸»é¡µ
    2. å¤šé‡ç­‰å¾…ç­–ç•¥ç¡®ä¿å†…å®¹åŠ è½½
    3. å¼ºåˆ¶æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
    """
    print(">>> [SYNC] å¼€å§‹æ‰§è¡Œå…¨é‡åŒæ­¥ä»»åŠ¡ (é­”æ”¹ç‰ˆ)...")
    
    for attempt in range(1, MAX_RETRIES + 1):
        print(f">>> [SYNC] ç¬¬ {attempt}/{MAX_RETRIES} æ¬¡å°è¯•...")
        page = await context.new_page()
        
        # âš ï¸ æš‚æ—¶ä¸ç”¨ stealthï¼Œæµ‹è¯•7æ˜¾ç¤ºstealthåè€Œåªè·å–1ç¯‡
        # if HAS_STEALTH: await stealth_async(page)
        
        try:
            # ============================================
            # ğŸ”¥ å…³é”®ä¿®æ”¹1ï¼šç›´æ¥è®¿é—®ç”¨æˆ·ä¸»é¡µï¼Œä¸è¦çƒ­èº«ï¼
            # æµ‹è¯•è¯æ˜ï¼šå…ˆè®¿é—®é¦–é¡µä¼šå¯¼è‡´é—®é¢˜
            # ============================================
            print("[SYNC] ğŸš€ ç›´æ¥è®¿é—®ç›®æ ‡ç”¨æˆ·ä¸»é¡µ (ä¸çƒ­èº«)...")
            
            # ä¼˜å…ˆä½¿ç”¨ networkidleï¼ˆæµ‹è¯•2è¯æ˜æœ‰æ•ˆï¼‰
            try:
                await page.goto(TOUTIAO_URL, wait_until="networkidle", timeout=45000)
                print("[SYNC] âœ“ networkidle å®Œæˆ")
            except Exception as timeout_err:
                # å¦‚æœ networkidle è¶…æ—¶ï¼Œé™çº§åˆ° domcontentloaded
                print(f"[SYNC] networkidle è¶…æ—¶ï¼Œé™çº§å¤„ç†: {timeout_err}")
                await page.goto(TOUTIAO_URL, wait_until="domcontentloaded", timeout=30000)
            
            # ============================================
            # ğŸ”¥ å…³é”®ä¿®æ”¹2ï¼šå¼ºåˆ¶ç­‰å¾… + æ£€æµ‹æ–‡ç« å…ƒç´ 
            # ============================================
            print("[SYNC] ç­‰å¾…é¡µé¢æ¸²æŸ“...")
            await asyncio.sleep(5)  # å›ºå®šç­‰å¾…5ç§’
            
            # éªŒè¯ç æ£€æŸ¥
            if await check_captcha(page, f"sync_try_{attempt}"):
                raise Exception("Captcha detected")
            
            # å°è¯•ç­‰å¾…æ–‡ç« å…ƒç´ å‡ºç°
            article_selectors = [
                'a[href*="/article/"]',
                'a[href*="/w/"]',
                'a[href*="/video/"]',
            ]
            
            element_found = False
            for sel in article_selectors:
                try:
                    await page.wait_for_selector(sel, timeout=8000)
                    print(f"[SYNC] âœ“ æ£€æµ‹åˆ°æ–‡ç« å…ƒç´ : {sel}")
                    element_found = True
                    break
                except:
                    continue
            
            if not element_found:
                print("[SYNC] âš  åˆå§‹æœªæ£€æµ‹åˆ°æ–‡ç« å…ƒç´ ï¼Œå°†é€šè¿‡æ»šåŠ¨è§¦å‘åŠ è½½...")
            
            # ============================================
            # ğŸ”¥ å…³é”®ä¿®æ”¹3ï¼šå¼ºåˆ¶æ»šåŠ¨åŠ è½½ï¼ˆå³ä½¿å·²æœ‰å†…å®¹ï¼‰
            # ============================================
            print("[SYNC] æ‰§è¡Œæ»šåŠ¨åŠ è½½...")
            
            all_links = []
            no_new_count = 0
            seen_urls = set()
            
            for scroll_round in range(MAX_SYNC_SCROLLS):
                # æå–å½“å‰é¡µé¢é“¾æ¥
                current_links = await page.evaluate(EXTRACT_LINKS_JS)
                
                # è®¡ç®—æ–°å¢æ•°é‡
                new_urls = [l for l in current_links if l['href'] not in seen_urls]
                for l in current_links:
                    seen_urls.add(l['href'])
                
                all_links = current_links
                
                print(f"[SYNC] æ»šåŠ¨ {scroll_round + 1}/{MAX_SYNC_SCROLLS}: "
                      f"å½“å‰ {len(all_links)} ç¯‡ (æœ¬è½®æ–°å¢ {len(new_urls)})")
                
                # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿæ–‡ç« ï¼Œå¯ä»¥æå‰ç»“æŸ
                if len(all_links) >= 40:
                    print("[SYNC] å·²è·å–è¶³å¤Ÿæ–‡ç« ï¼Œæå‰ç»“æŸæ»šåŠ¨")
                    break
                
                # æ£€æµ‹æ˜¯å¦è¿˜æœ‰æ–°å†…å®¹åŠ è½½
                if len(new_urls) == 0:
                    no_new_count += 1
                    if no_new_count >= 4:  # è¿ç»­4æ¬¡æ— æ–°å†…å®¹
                        print("[SYNC] è¿ç»­4æ¬¡æ— æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨")
                        break
                else:
                    no_new_count = 0
                
                # æ»šåŠ¨æ“ä½œï¼ˆæ¨¡æ‹ŸçœŸå®ç”¨æˆ·ï¼‰
                scroll_distance = random.randint(400, 700)
                await page.mouse.wheel(0, scroll_distance)
                await asyncio.sleep(random.uniform(1.5, 2.5))
                
                # å¶å°”å›æ»šä¸€ä¸‹ï¼ˆæ›´åƒçœŸäººï¼‰
                if random.random() < 0.15:
                    await page.mouse.wheel(0, -random.randint(100, 200))
                    await asyncio.sleep(0.5)
            
            # æœ€ç»ˆç­‰å¾…
            await asyncio.sleep(2)
            
            # æœ€ç»ˆæå–
            final_links = await page.evaluate(EXTRACT_LINKS_JS)
            print(f"[SYNC] æœ€ç»ˆæå–: {len(final_links)} ç¯‡")
            
            # ============================================
            # ç»“æœåˆ¤æ–­
            # ============================================
            if final_links and len(final_links) > 0:
                # ========== æˆåŠŸ ==========
                print(f"[SYNC] âœ… åŒæ­¥æˆåŠŸ! å…± {len(final_links)} ç¯‡æ–‡ç« ")
                
                # ä¿å­˜æˆåŠŸæˆªå›¾
                await page.screenshot(path=DEBUG_DIR / "sync_success_latest.png")
                
                # æ‰“å°å‰5ç¯‡æ–‡ç« æ ‡é¢˜ï¼ˆéªŒè¯ï¼‰
                print("[SYNC] æ–‡ç« æ ·æœ¬:")
                for i, link in enumerate(final_links[:5], 1):
                    print(f"       {i}. {link['text'][:40]}...")
                
                # å†™å…¥æ•°æ®åº“
                db.add_articles(final_links)
                db.mark_synced()
                
                # æ¸…ç†æ—§çš„é”™è¯¯æ–‡ä»¶
                print("[SYNC] æ¸…ç†æ—§çš„è°ƒè¯•/é”™è¯¯æ–‡ä»¶...")
                try:
                    for pattern in ["error_sync_*.png", "debug_sync_fail_*.png", 
                                    "sync_source_*.html", "captcha_sync_*.png"]:
                        for file_path in DEBUG_DIR.glob(pattern):
                            file_path.unlink(missing_ok=True)
                except Exception as clean_err:
                    print(f"[WARN] æ¸…ç†æ–‡ä»¶å¤±è´¥: {clean_err}")
                
                await page.close()
                return  # æˆåŠŸé€€å‡º
                
            else:
                # ========== å¤±è´¥ ==========
                print(f"[SYNC] âŒ ç¬¬ {attempt} æ¬¡æœªèƒ½æå–åˆ°æ–‡ç« ")
                
                # ä¿å­˜è°ƒè¯•ä¿¡æ¯
                await page.screenshot(path=DEBUG_DIR / f"debug_sync_fail_attempt_{attempt}.png")
                
                try:
                    html_content = await page.content()
                    (DEBUG_DIR / f"sync_source_attempt_{attempt}.html").write_text(
                        html_content, encoding="utf-8"
                    )
                except:
                    pass
                
                raise Exception("No articles extracted after scrolling")
                
        except Exception as e:
            print(f"[SYNC] ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {e}")
            
            # ä¿å­˜é”™è¯¯æˆªå›¾
            try:
                if not page.is_closed():
                    await page.screenshot(path=DEBUG_DIR / f"error_sync_attempt_{attempt}.png")
            except:
                pass
            
            if attempt == MAX_RETRIES:
                print("[FATAL] âŒ å…¨é‡åŒæ­¥ä»»åŠ¡æœ€ç»ˆå¤±è´¥ã€‚")
                # ä¿å­˜æœ€ç»ˆHTMLç”¨äºåˆ†æ
                try:
                    if not page.is_closed():
                        final_html = await page.content()
                        (DEBUG_DIR / "sync_source_final_fail.html").write_text(
                            final_html, encoding="utf-8"
                        )
                except:
                    pass
            else:
                # é‡è¯•å‰ç­‰å¾…
                wait_time = random.randint(5, 10)
                print(f"[SYNC] ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)
        
        finally:
            try:
                if not page.is_closed():
                    await page.close()
            except:
                pass
    
    print("[SYNC] âŒ å…¨é‡åŒæ­¥ä»»åŠ¡å®Œå…¨å¤±è´¥")

async def read_article_task(context: BrowserContext, article: dict, db: ArticleDB):
    """
    å•ç¯‡é˜…è¯»ä»»åŠ¡ï¼šåŒ…å«ä¼˜åŒ–çš„æ—¶é•¿ç®—æ³•
    æµç¨‹ï¼šä¸»é¡µçƒ­èº« -> ç”¨æˆ·ä¸»é¡µ -> æ–‡ç« é¡µ (æ¨¡æ‹Ÿä»ä½œè€…ä¸»é¡µç‚¹å‡»è¿›å…¥) -> æˆªå›¾å­˜æ¡£ -> é˜…è¯» -> ç»“æŸ
    """
    url = article['url']
    title_preview = article['title'][:20]
    print(f"--- [READ] å‡†å¤‡é˜…è¯»: {title_preview}... ---")
    
    page = await context.new_page()
    if HAS_STEALTH: await stealth_async(page)

    try:
        # 1. è®¿é—®ä¸»é¡µ (æ¨¡æ‹Ÿç”¨æˆ·æ‰“å¼€APP/ç½‘ç«™)
        print("[READ] æ­¥éª¤1: è®¿é—®ä¸»é¡µ...")
        await page.goto("https://www.toutiao.com/", wait_until="domcontentloaded", timeout=30000)
        await human_delay(1.5, 3)

        # 2. è®¿é—®ä½œè€…ä¸»é¡µ (æ¨¡æ‹Ÿç‚¹å‡»å¤´åƒè¿›å…¥)
        print("[READ] æ­¥éª¤2: è¿›å…¥ä½œè€…ä¸»é¡µ...")
        await page.goto(TOUTIAO_URL, wait_until="domcontentloaded", timeout=45000)
        await human_delay(2, 4)

        # 3. è¿›å…¥å…·ä½“æ–‡ç« é¡µ
        print(f"[READ] æ­¥éª¤3: æ‰“å¼€æ–‡ç« é¡µé¢...")
        await page.goto(url, wait_until="domcontentloaded", timeout=45000)
        
        # 4. éªŒè¯ç ä¸404æ£€æŸ¥
        await human_delay(2, 3)
        if await check_captcha(page, "read"):
            return

        # ============ å¢åŠ ï¼šæ–‡ç« æ‰“å¼€éªŒè¯æˆªå›¾ ============
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ç®€å•çš„åºå·å‘½åï¼Œæ–¹ä¾¿æŸ¥çœ‹
        screenshot_name = f"read_verify_{timestamp_str}.png"
        await page.screenshot(path=DEBUG_DIR / screenshot_name)
        print(f"[READ] å·²ä¿å­˜é˜…è¯»éªŒè¯æˆªå›¾: {screenshot_name}")
        # ==============================================

        page_content = await page.evaluate("document.body.innerText")
        page_title = await page.title()
        
        # ç®€æ˜“çš„å¤±æ•ˆåˆ¤æ–­
        invalid_keywords = ["404", "é¡µé¢ä¸å­˜åœ¨", "æ–‡ç« å·²åˆ é™¤", "å‚æ•°é”™è¯¯"]
        if any(k in page_title for k in invalid_keywords):
            print("[READ] æ–‡ç« å·²å¤±æ•ˆï¼Œæ ‡è®° invalidã€‚")
            db.mark_invalid(url)
            return

        # =========================================================
        # æ ¸å¿ƒä¿®æ”¹ï¼šä¼˜åŒ–é˜…è¯»æ—¶é•¿è®¡ç®—ç®—æ³•
        # ç›®æ ‡ï¼š30s ~ 180s è‡ªç„¶åˆ†å¸ƒï¼Œé¿å…ä¸€åˆ€åˆ‡
        # =========================================================
        
        # 1. å­—æ•°ç»Ÿè®¡
        word_count = len(page_content)
        
        # 2. å›¾ç‰‡æ•°é‡ç»Ÿè®¡
        img_count = await page.evaluate("""
            () => {
                const imgs = document.querySelectorAll('article img, .tt-input__content img, .article-content img, .pgc-img img');
                return imgs.length;
            }
        """)

        # 3. è®¡ç®—åŸºå‡†æ—¶é•¿
        text_time = word_count / 25.0  
        img_time = img_count * 5.0
        base_time = text_time + img_time
        
        if base_time < 10:
            base_time = random.randint(20, 40)
        
        # 4. å¢åŠ éšæœºæ‰°åŠ¨
        variation = random.gauss(1.0, 0.2)
        thinking_time = random.uniform(5, 15)
        
        # è®¡ç®—æ€»æ—¶é•¿
        calc_seconds = (base_time * variation) + thinking_time
        read_seconds = max(30.0, calc_seconds)
        read_seconds = min(180.0, read_seconds)
        
        print(f"[READ] å­—æ•°:{word_count} | å›¾ç‰‡:{img_count} | ç®—æ³•è®¡ç®—:{calc_seconds:.1f}s")
        print(f"[READ] >> æœ€ç»ˆè®¡åˆ’åœç•™: {read_seconds:.1f}ç§’")
        
        # =========================================================

        # äº¤äº’å¾ªç¯
        start_read = time.time()
        while (time.time() - start_read) < read_seconds:
            # éšæœºä¸‹æ»‘
            await human_scroll(page, max_scrolls=1)
            
            # éšæœºé¼ æ ‡ç§»åŠ¨
            if random.random() < 0.3:
                await human_mouse_move(page, random.randint(200, 1000), random.randint(300, 800))
            
            # æä½æ¦‚ç‡æ¨¡æ‹Ÿé€‰ä¸­æ–‡æœ¬
            if random.random() < 0.1:
                try:
                    await page.click("p", timeout=200)
                except: pass

        # å¿…é¡»åŠ¨ä½œï¼šæ»‘åŠ¨åˆ°åº•éƒ¨
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await human_delay(1.5, 3.0)
        
        # æˆåŠŸå®Œæˆ
        print(f"[READ] é˜…è¯»å®Œæˆã€‚")
        db.record_read(url)

    except Exception as e:
        print(f"[READ] å¼‚å¸¸: {e}")
        await page.screenshot(path=DEBUG_DIR / "error_read_latest.png")
    finally:
        await page.close()

# ================= ä¸»ç¨‹åºå…¥å£ =================

async def main():
    # 1. å‡†å¤‡å·¥ä½œ
    db = ArticleDB(DB_FILE)
    
    # éšæœºé€‰æ‹©è§†çª—
    vp = random.choice(VIEWPORTS)
    # è·å–éšæœº UA
    ua = get_pc_user_agent()
    
    print(f"[INIT] å¯åŠ¨çˆ¬è™«ä»»åŠ¡")
    print(f"[INIT] UA: {ua[:50]}...")
    print(f"[INIT] Viewport: {vp['width']}x{vp['height']}")

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        # ç”Ÿäº§ç¯å¢ƒä¿æŒ headless=True
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled", # å»é™¤è‡ªåŠ¨åŒ–ç‰¹å¾
                "--no-sandbox",
                "--disable-infobars",
                "--window-size={},{}".format(vp['width'], vp['height'])
            ]
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = await browser.new_context(
            user_agent=ua,
            viewport=vp,
            locale="zh-CN",
            timezone_id="Asia/Shanghai",
            device_scale_factor=1,
            has_touch=False,
            is_mobile=False,
            java_script_enabled=True
        )

        # æ³¨å…¥ webdriver ç§»é™¤è„šæœ¬ (åŒé‡ä¿é™©)
        await context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        # --- æ­¥éª¤ 1: æ£€æŸ¥æ˜¯å¦éœ€è¦å…¨é‡åŒæ­¥ ---
        # å¦‚æœä»Šå¤©æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œæˆ–è€…æ•°æ®åº“ä¸ºç©ºï¼Œåˆ™æ‰§è¡ŒåŒæ­¥
        if db.needs_sync() or not db.data.get("articles"):
            await sync_task(context, db)
        else:
            print("[INIT] ä»Šæ—¥å·²æ‰§è¡Œè¿‡åŒæ­¥ï¼Œè·³è¿‡åˆ—è¡¨æŠ“å–ã€‚")

        # --- æ­¥éª¤ 2: è·å–ä»Šæ—¥é˜…è¯»ç›®æ ‡ ---
        targets = db.get_weighted_candidates()
        
        if not targets:
            print("[DONE] æš‚æ— å¾…è¯»æ–‡ç«  (å¯èƒ½å·²å…¨éƒ¨è¯»å®Œæˆ–æ— æ–°å†…å®¹)ã€‚")
            await browser.close()
            return

        # --- æ­¥éª¤ 3: å¾ªç¯é˜…è¯» ---
        # æ³¨æ„ï¼šä¸å†éœ€è¦å•ç‹¬çš„é¦–é¡µçƒ­èº«ï¼Œå› ä¸º read_article_task å†…éƒ¨å·²ç»åŒ…å«äº†æµç¨‹
        for i, article in enumerate(targets, 1):
            print(f"\n>>> è¿›åº¦ [{i}/{len(targets)}]")
            await read_article_task(context, article, db)
            
            # ç¯‡é—´å†·å´æ—¶é—´ (é¿å…æ“ä½œè¿‡å¿«)
            if i < len(targets):
                wait_time = random.randint(8, 15)
                print(f"[COOL] ä¼‘æ¯ {wait_time} ç§’...")
                await asyncio.sleep(wait_time)

        await browser.close()
        print("\n[DONE] æ‰€æœ‰ä»»åŠ¡å®Œæˆã€‚")

if __name__ == "__main__":
    asyncio.run(main())
