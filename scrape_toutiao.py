import asyncio
import json
import random
import time
import math
import sys
from datetime import datetime
from pathlib import Path

from playwright.async_api import async_playwright, Page, BrowserContext

# ================= ä¾èµ–åº“æ£€æµ‹ =================
try:
    # å°è¯•å¯¼å…¥ playwright-stealth å¢å¼ºé˜²çˆ¬èƒ½åŠ›
    from playwright_stealth import stealth_async
    HAS_STEALTH = True
except ImportError:
    HAS_STEALTH = False
    print("================================================================")
    print(f"[WARN] æœªå®‰è£… playwright-stealth åº“ã€‚")
    print(f"[WARN] å»ºè®®è¿è¡Œ: pip install playwright-stealth ä»¥é™ä½è¢«æ£€æµ‹é£é™©ã€‚")
    print("================================================================")

# ================= é…ç½®åŒºåŸŸ =================

# ç›®æ ‡ç”¨æˆ·ä¸»é¡µ Token URL (è¯·ç¡®ä¿æ­¤é“¾æ¥æœ‰æ•ˆ)
TOUTIAO_URL = "https://www.toutiao.com/c/user/token/CiyRLPHkUyTCD9FmHodOGQVcmZh5-NRKyfiTSF0XMms-tSja0FdhrUWRp-T-DBpJCjwAAAAAAAAAAAAAT8lExjCbDHcWTgszQQjqU0Ohh9qtuXbuEOe6CQdqJEZ7yIpoM-NJ93_Sty1iMpOe_FUQ9ZmDDhjDxYPqBCIBA9GPpzc="

# è¾“å‡ºè®¾ç½®
DATA_DIR = Path("data")
DATA_DIR.mkdir(parents=True, exist_ok=True)

DB_FILE = DATA_DIR / "toutiao_db.json"
DEBUG_DIR = DATA_DIR / "debug"
DEBUG_DIR.mkdir(parents=True, exist_ok=True)

# æµè§ˆè¡Œä¸ºé™åˆ¶
MAX_READ_COUNT = 10     # æ¯æ¬¡è¿è¡Œè„šæœ¬æœ€å¤šé˜…è¯»å¤šå°‘ç¯‡
MIN_READ_COUNT = 3      # æ¯æ¬¡è¿è¡Œè„šæœ¬æœ€å°‘é˜…è¯»å¤šå°‘ç¯‡
MAX_SYNC_SCROLLS = 20   # åŒæ­¥åˆ—è¡¨æ—¶æœ€å¤§ä¸‹æ»‘æ¬¡æ•°
AGING_THRESHOLD = 50    # æ–‡ç« â€œè€åŒ–â€é˜ˆå€¼
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°


# ================= User-Agent ç®¡ç† =================

# å†…ç½®å…œåº• PC UA åº“ (è¦†ç›–ä¸»æµæµè§ˆå™¨ä¸æ“ä½œç³»ç»Ÿ)
FALLBACK_PC_UAS = [
    # Windows 10/11 Chrome
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    # Windows Edge
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
    # Mac Chrome
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, Like Gecko) Chrome/119.0.0.0 Safari/537.36",
    # Mac Safari
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    # Linux Chrome
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

def get_pc_user_agent():
    """
    ä¼˜å…ˆä½¿ç”¨ real-useragent åº“è·å–éšæœº PC UAã€‚
    å¦‚æœè·å–å¤±è´¥æˆ–åº“æœªå®‰è£…ï¼Œä½¿ç”¨å†…ç½®åˆ—è¡¨å…œåº•ã€‚
    """
    ua = ""
    try:
        from real_useragent import UserAgent
        rua = UserAgent()
        ua = rua.desktop_useragent()
        # ç®€å•æ ¡éªŒè·å–çš„UAæ˜¯å¦åˆæ³•
        if not ua or len(ua) < 20:
            raise ValueError("UA too short")
    except Exception:
        ua = random.choice(FALLBACK_PC_UAS)
    
    return ua

# å¸¸è§ PC åˆ†è¾¨ç‡åº“ (é¿å…å•ä¸€æŒ‡çº¹)
VIEWPORTS = [
    {"width": 1920, "height": 1080},
    {"width": 1536, "height": 864},
    {"width": 1440, "height": 900},
    {"width": 1366, "height": 768},
    {"width": 1280, "height": 720},
]

# ================= JS æ³¨å…¥è„šæœ¬ (æ ¸å¿ƒé€»è¾‘ä¼˜åŒ–) =================
# å¢å¼ºç‰ˆé“¾æ¥æå–è„šæœ¬ï¼šå…¼å®¹æ€§ä¿®å¤(ç§»é™¤?.) + å¢å¼ºæ ‡é¢˜æå– + è¿‡æ»¤è¯æ›´æ–°
EXTRACT_LINKS_JS = r"""
() => {
  const anchors = Array.from(document.querySelectorAll("a[href]"));
  const origin = window.location.origin;
  const results = [];
  const seen = new Set();
  
  // 1. è·¯å¾„ç‰¹å¾åˆ¤æ–­
  const isArticle = (path) => {
    if (!path) return false;
    if (path.startsWith("/c/user/")) return false;
    if (path.startsWith("/search/")) return false;
    if (path.includes("toutiao_search")) return false;

    const lastPart = path.split("/").filter(Boolean).pop();
    if (!lastPart) return false;
    const digits = lastPart.replace(/\D/g, "").length;
    return digits > 5;
  };

  // 2. åŸºç¡€æ–‡æœ¬æå–
  const getText = (el) => {
    if (!el) return "";
    let txt = (el.innerText || "").trim();
    if (txt) return txt;
    
    txt = (el.getAttribute("aria-label") || "").trim();
    if (txt) return txt;
    
    txt = (el.getAttribute("title") || "").trim();
    if (txt) return txt;
    
    const img = el.querySelector("img");
    if (img) {
        txt = (img.getAttribute("alt") || "").trim();
    }
    return txt;
  };

  // 3. æˆªå–æ–‡æœ¬
  const truncateText = (text, maxLength = 50) => {
    if (!text || text.length <= maxLength) return text;
    return text.substring(0, maxLength) + "...";
  };

  // 4. è·å–å†…å®¹ç±»å‹
  const getContentType = (url) => {
    if (url.includes("/article/")) return "article";
    if (url.includes("/w/")) return "weitoutiao";
    if (url.includes("/video/")) return "video";
    return "unknown";
  };

  // 5. å¢å¼ºçš„æ ‡é¢˜æå–
  const extractTitle = (a, urlObj) => {
    let text = getText(a);
    const contentType = getContentType(urlObj.pathname);

    // å¦‚æœç›´æ¥è·å–å¤±è´¥æˆ–æ–‡æœ¬å¤ªçŸ­ï¼Œå°è¯•æ›´å¤šæ–¹æ³•
    if (!text || text.length < 4) {
        let container = a.closest('.feed-card-wrapper, .article-card, .feed-card-article-wrapper, .card-wrapper, .weitoutiao-wrap, .wtt-content');
        
        // --- å…¼å®¹æ€§ä¿®æ”¹ï¼šä¸ä½¿ç”¨ ?. æ“ä½œç¬¦ ---
        if (!container) {
            if (a.parentElement && a.parentElement.parentElement && a.parentElement.parentElement.parentElement) {
                container = a.parentElement.parentElement.parentElement;
            }
        }

        if (container) {
            // å¾®å¤´æ¡ç­–ç•¥
            if (contentType === "weitoutiao") {
                const contentEl = container.querySelector('.weitoutiao-content, .wtt-content, .feed-card-article-content, [class*="content"]');
                if (contentEl) {
                    const content = contentEl.innerText.trim();
                    if (content) {
                        text = truncateText(content, 40);
                    }
                }
            } 
            // è§†é¢‘ç­–ç•¥
            else if (contentType === "video") {
                const titleEl = container.querySelector('.video-title, .title, [class*="title"]');
                if (titleEl) {
                    const t = titleEl.innerText.trim();
                    if (t) text = t;
                }
            }

            // é€šç”¨æ ‡é¢˜æŸ¥æ‰¾
            if (!text || text.length < 4) {
                const selectors = [
                    '.title', '.feed-card-article-title', '.article-title', '.feed-card-article-l a',
                    '[class*="title"]', 'h1, h2, h3', '.text', 'p'
                ];

                for (const selector of selectors) {
                    const el = container.querySelector(selector);
                    if (el) {
                        const t = el.innerText.trim();
                        if (t && t.length > 4) {
                            text = truncateText(t, 50);
                            break;
                        }
                    }
                }
            }

            // æœ€åå°è¯•ï¼šè·å–å®¹å™¨å†…ç¬¬ä¸€ä¸ªé•¿æ–‡æœ¬
            if (!text || text.length < 4) {
                const allTexts = container.innerText.trim().split('\n').filter(t => t.trim().length > 4);
                if (allTexts.length > 0) {
                    text = truncateText(allTexts[0], 50);
                }
            }
        }
    }

    // å…œåº•é‡å‘½å
    if (!text || text === "Untitled") {
        if (contentType === "weitoutiao") text = "[å¾®å¤´æ¡]";
        else if (contentType === "video") text = "[è§†é¢‘]";
    }

    return { text: text || "Untitled", contentType };
  };

  // ä¸»å¾ªç¯
  for (const a of anchors) {
    let href = a.getAttribute("href");
    if (!href) continue;
    
    if (href.startsWith("/")) href = origin + href;
    
    try {
        const urlObj = new URL(href);
        
        if (!urlObj.hostname.includes("toutiao.com")) continue;
        if (!isArticle(urlObj.pathname)) continue;
        
        const cleanUrl = urlObj.origin + urlObj.pathname;
        if (seen.has(cleanUrl)) continue;

        const titleInfo = extractTitle(a, urlObj);
        let text = titleInfo.text;

        // å…³é”®è¯è¿‡æ»¤ (å·²æ·»åŠ 'ä¾µæƒä¸¾æŠ¥å—ç†å…¬ç¤º')
        const filterKeywords = [
            'è·Ÿå¸–è¯„è®ºè‡ªå¾‹ç®¡ç†æ‰¿è¯ºä¹¦',
            'ç”¨æˆ·åè®®',
            'éšç§æ”¿ç­–',
            'ä¾µæƒæŠ•è¯‰',
            'ç½‘ç»œè°£è¨€æ›å…‰å°',
            'è¿æ³•å’Œä¸è‰¯ä¿¡æ¯ä¸¾æŠ¥',
            'ä¾µæƒä¸¾æŠ¥å—ç†å…¬ç¤º'
        ];
        
        if (filterKeywords.some(keyword => text.includes(keyword))) continue;

        // é¢å¤–çš„çŸ­è¯è¿‡æ»¤
        if (!text.startsWith('[') && text.match(/^(å¤‡æ¡ˆ|ä¸¾æŠ¥|ç™»å½•|ä¸‹è½½|å¹¿å‘Š|ç›¸å…³æ¨è|æœç´¢)$/)) continue;
        
        seen.add(cleanUrl);
        results.push({ 
            text: text, 
            href: cleanUrl,
            type: titleInfo.contentType
        });

    } catch(e) {}
  }

  return results;
}
"""

# ================= æ•°æ®åº“ç®¡ç†ç±» =================

class ArticleDB:
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.data = self._load()

    def _load(self):
        if not self.db_path.exists():
            return {"last_sync_date": "", "articles": {}}
        try:
            return json.loads(self.db_path.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[DB] è¯»å–æ•°æ®åº“å‡ºé”™: {e}ï¼Œå°†åˆå§‹åŒ–æ–°åº“")
            return {"last_sync_date": "", "articles": {}}

    def save(self):
        try:
            self.db_path.write_text(json.dumps(self.data, indent=2, ensure_ascii=False), encoding="utf-8")
        except Exception as e:
            print(f"[DB] ä¿å­˜å¤±è´¥: {e}")

    def needs_sync(self) -> bool:
        """åˆ¤æ–­ä»Šå¤©æ˜¯å¦å·²ç»æ‰§è¡Œè¿‡å…¨é‡æŠ“å–"""
        today = datetime.now().strftime("%Y-%m-%d")
        return self.data.get("last_sync_date") != today

    def mark_synced(self):
        self.data["last_sync_date"] = datetime.now().strftime("%Y-%m-%d")
        self.save()

    def add_articles(self, scraped_items: list):
        """å¢é‡æ·»åŠ æ–‡ç« """
        added_count = 0
        current_urls = self.data["articles"]
        
        for item in scraped_items:
            url = item['href']
            # å¦‚æœæ˜¯æ–°é“¾æ¥ï¼Œæˆ–è€…æ—§é“¾æ¥æ˜¯Untitledä½†è¿™æ¬¡æŠ“åˆ°äº†çœŸæ ‡é¢˜ï¼Œåˆ™æ›´æ–°
            if url not in current_urls:
                current_urls[url] = {
                    "title": item['text'],
                    "url": url,
                    "status": "active",
                    "last_read_at": "",
                    "read_count": 0
                }
                added_count += 1
            elif current_urls[url]["title"] == "Untitled" and item['text'] != "Untitled":
                 current_urls[url]["title"] = item['text'] # ä¿®æ­£æ ‡é¢˜
        
        print(f"[DB] æ•°æ®åº“æ›´æ–°: æ–°å¢ {added_count} ç¯‡ï¼Œå½“å‰æ€»åº“å­˜ {len(current_urls)} ç¯‡")
        self.save()

    def mark_invalid(self, url):
        """æ ‡è®°å¤±æ•ˆ"""
        if url in self.data["articles"]:
            self.data["articles"][url]["status"] = "invalid"
            print(f"[DB] é“¾æ¥æ ‡è®°ä¸ºæ— æ•ˆ: {url}")
            self.save()

    def record_read(self, url):
        """è®°å½•é˜…è¯»"""
        if url in self.data["articles"]:
            today = datetime.now().strftime("%Y-%m-%d")
            entry = self.data["articles"][url]
            entry["last_read_at"] = today
            entry["read_count"] = entry.get("read_count", 0) + 1
            self.save()

    def get_weighted_candidates(self) -> list:
        """è·å–ä»Šæ—¥é˜…è¯»åˆ—è¡¨ï¼šæƒé‡ç®—æ³•"""
        today = datetime.now().strftime("%Y-%m-%d")
        candidates = []
        weights = []
        
        active_urls = [k for k, v in self.data["articles"].items() if v.get("status") == "active"]
        
        for url in active_urls:
            info = self.data["articles"][url]
            
            # è§„åˆ™1: ä»Šå¤©è¯»è¿‡çš„ç»å¯¹ä¸è¯»
            if info.get("last_read_at") == today:
                continue
            
            read_count = info.get("read_count", 0)
            
            # è§„åˆ™2: æƒé‡è®¡ç®—
            # æ²¡è¯»è¿‡çš„(0æ¬¡): æé«˜æƒé‡ 200
            # è¯»å¾—å°‘çš„(<5æ¬¡): é«˜æƒé‡ 100
            # æ™®é€š(<20æ¬¡): ä¸­æƒé‡ 50
            # è€æ—§(>50æ¬¡): ä½æƒé‡ 5 (ä¿ç•™å¾®å°æ¦‚ç‡)
            if read_count == 0:
                w = 200
            elif read_count < 5:
                w = 100
            elif read_count < 20:
                w = 50
            elif read_count < AGING_THRESHOLD:
                w = 20
            else:
                w = 5
            
            candidates.append(info)
            weights.append(w)
            
        if not candidates:
            return []

        # æ— æ”¾å›æŠ½å–
        target_k = random.randint(MIN_READ_COUNT, MAX_READ_COUNT)
        target_k = min(target_k, len(candidates))
        
        print(f"[PLAN] å¯é€‰æ–‡ç« åº“: {len(candidates)} ç¯‡. è®¡åˆ’é˜…è¯»: {target_k} ç¯‡")
        
        selected = []
        temp_cand = list(candidates)
        temp_weight = list(weights)
        
        for _ in range(target_k):
            if not temp_cand: break
            chosen = random.choices(temp_cand, weights=temp_weight, k=1)[0]
            selected.append(chosen)
            
            idx = temp_cand.index(chosen)
            temp_cand.pop(idx)
            temp_weight.pop(idx)
            
        return selected

# ================= æ‹ŸäººåŒ–æ“ä½œå‡½æ•° =================

async def human_delay(min_s=1.0, max_s=3.0):
    """å¸¦éšæœºæ€§çš„ç­‰å¾…"""
    await asyncio.sleep(random.uniform(min_s, max_s))

async def human_mouse_move(page: Page, x_target, y_target, steps=25):
    """è´å¡å°”æ›²çº¿æ¨¡æ‹Ÿé¼ æ ‡ç§»åŠ¨"""
    try:
        start_x = random.randint(100, 1000)
        start_y = random.randint(100, 600)
        
        ctrl_x1 = start_x + (x_target - start_x) * 0.3 + random.randint(-50, 50)
        ctrl_y1 = start_y + (y_target - start_y) * 0.3 + random.randint(-50, 50)
        ctrl_x2 = start_x + (x_target - start_x) * 0.7 + random.randint(-50, 50)
        ctrl_y2 = start_y + (y_target - start_y) * 0.7 + random.randint(-50, 50)

        for i in range(steps + 1):
            t = i / steps
            x = (1-t)**3 * start_x + 3*(1-t)**2 * t * ctrl_x1 + 3*(1-t)*t**2 * ctrl_x2 + t**3 * x_target
            y = (1-t)**3 * start_y + 3*(1-t)**2 * t * ctrl_y1 + 3*(1-t)*t**2 * ctrl_y2 + t**3 * y_target
            
            # æŠ–åŠ¨
            x += random.uniform(-2, 2)
            y += random.uniform(-2, 2)
            
            await page.mouse.move(x, y)
            await asyncio.sleep(random.uniform(0.005, 0.015))
    except Exception:
        pass

async def human_scroll(page: Page, max_scrolls=1):
    """æ‹ŸäººåŒ–æ»šåŠ¨"""
    for _ in range(max_scrolls):
        # éšæœºæ»šåŠ¨å¹…åº¦
        delta_y = random.randint(300, 700)
        await page.mouse.wheel(0, delta_y)
        
        # æ»šåŠ¨åçš„åœé¡¿ï¼Œæ¨¡æ‹Ÿé˜…è¯»
        await human_delay(1.0, 2.5)
        
        # 20% æ¦‚ç‡å›æ»š (å›çœ‹)
        if random.random() < 0.2:
            await page.mouse.wheel(0, -random.randint(100, 250))
            await human_delay(0.5, 1.2)

async def check_captcha(page: Page, tag="unknown") -> bool:
    """æ£€æŸ¥éªŒè¯ç ï¼Œå¹¶æˆªå›¾ï¼ˆè¦†ç›–æœ€æ–°ä¸€ä»½ï¼‰"""
    try:
        title = await page.title()
        is_captcha = False
        
        # 1. æ ‡é¢˜åˆ¤æ–­
        if any(kw in title for kw in ["éªŒè¯", "å®‰å…¨æ£€æµ‹", "captcha", "verify"]):
            is_captcha = True
            
        # 2. DOM åˆ¤æ–­
        if not is_captcha:
            if await page.query_selector("#captcha-verify-image") or \
               await page.query_selector(".captcha_verify_container"):
                is_captcha = True
        
        if is_captcha:
            print(f"[ALERT] {tag} é˜¶æ®µæ£€æµ‹åˆ°éªŒè¯ç ! Title: {title}")
            # ä¿å­˜éªŒè¯ç æˆªå›¾ï¼Œè¦†ç›–æ—§çš„åŒç±»å‹æ–‡ä»¶
            screenshot_path = DEBUG_DIR / f"captcha_{tag}_latest.png"
            await page.screenshot(path=screenshot_path)
            print(f"[ALERT] éªŒè¯ç æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
            return True
            
        return False
    except Exception as e:
        print(f"[WARN] éªŒè¯ç æ£€æµ‹å‡ºé”™: {e}")
        return False

# ================= æ ¸å¿ƒä»»åŠ¡é€»è¾‘ =================

async def sync_task(context: BrowserContext, db: ArticleDB):
    """
    å…¨é‡åŒæ­¥ä»»åŠ¡ - æ™ºèƒ½ç­–ç•¥ä¼˜åŒ–ç‰ˆ
    
    é€»è¾‘è¯´æ˜:
    1. æ£€æŸ¥ DB åº“å­˜:
       - å¦‚æœæ˜¯åˆæ¬¡è¿è¡Œ(åº“å­˜ä¸º0)æˆ–åº“å­˜å¾ˆå°‘(<20): æ¿€æ´» [æ·±åº¦æŠ“å–æ¨¡å¼]ï¼Œå°½å¯èƒ½å¤šæ»‘ã€‚
       - å¦‚æœåº“å­˜å……è¶³: æ¿€æ´» [å¢é‡æ›´æ–°æ¨¡å¼]ï¼Œæ»‘åˆ°ä¸€å®šæ•°é‡å³åœæ­¢ï¼ŒèŠ‚çœèµ„æºã€‚
    2. åŠ¨æ€è°ƒæ•´: æ ¹æ®æ¨¡å¼ä¸åŒï¼Œè°ƒæ•´ æœ€å¤§æ»šåŠ¨æ¬¡æ•°(max_scrolls) å’Œ ç›®æ ‡æŠ“å–æ•°é‡(target_count)ã€‚
    3. å…œåº•æœºåˆ¶: æ— è®ºå“ªç§æ¨¡å¼ï¼Œå¦‚æœè¿ç»­å¤šæ¬¡æ»šä¸åˆ°æ–°å†…å®¹ï¼Œéƒ½ä¼šè‡ªåŠ¨åœæ­¢ã€‚
    """
    print(">>> [SYNC] å¼€å§‹æ‰§è¡Œå…¨é‡åŒæ­¥ä»»åŠ¡...")

    # ================= ç­–ç•¥åˆ¤å®šåŒºåŸŸ =================
    current_article_count = len(db.data.get("articles", {}))
    
    # åˆ¤å®šé˜ˆå€¼ï¼šå¦‚æœåº“å­˜å°‘äº 20 ç¯‡ï¼Œè§†ä¸ºå†·å¯åŠ¨/æ•°æ®ä¸è¶³
    IS_COLD_START = current_article_count < 20
    
    if IS_COLD_START:
        print(f">>> [STRATEGY] æ£€æµ‹åˆ°åº“å­˜è¾ƒå°‘ ({current_article_count} ç¯‡)ï¼Œæ¿€æ´»ã€æ·±åº¦æŠ“å–æ¨¡å¼ã€‘")
        # æ·±åº¦æ¨¡å¼ï¼šæ»šå¾—æ·±ï¼ŒæŠ“å¾—å¤š
        MAX_SCROLL_ROUNDS_DYNAMIC = 100   # æ­¤æ—¶æœ€å¤§å…è®¸æ»š 100 æ¬¡
        TARGET_ARTICLE_COUNT = 300        # ç›®æ ‡æŠ“å¤Ÿ 300 ç¯‡æ‰åœï¼ˆé™¤éåˆ°åº•ï¼‰
    else:
        print(f">>> [STRATEGY] æ£€æµ‹åˆ°åº“å­˜å……è¶³ ({current_article_count} ç¯‡)ï¼Œæ¿€æ´»ã€å¢é‡æ›´æ–°æ¨¡å¼ã€‘")
        # å¢é‡æ¨¡å¼ï¼šæ»šå¾—æµ…ï¼Œåªè¦æœ€æ–°çš„
        MAX_SCROLL_ROUNDS_DYNAMIC = 20    # å¹³æ—¶åªæ»š 20 æ¬¡
        TARGET_ARTICLE_COUNT = 40         # åªè¦æŠ“åˆ° 40 ç¯‡æ–°çƒ­æ–‡å°±å¤Ÿäº†
    # ===============================================

    for attempt in range(1, MAX_RETRIES + 1):
        print(f">>> [SYNC] ç¬¬ {attempt}/{MAX_RETRIES} æ¬¡å°è¯•è¿æ¥...")
        page = await context.new_page()
        
        try:
            # ============================================
            # 1. è®¿é—®ä¸»é¡µ
            # ============================================
            print("[SYNC] ğŸš€ è®¿é—®ç›®æ ‡ç”¨æˆ·ä¸»é¡µ...")
            try:
                # ä¼˜å…ˆç­‰å¾…ç½‘ç»œç©ºé—²ï¼Œç¡®ä¿ AJAX å†…å®¹åŠ è½½
                await page.goto(TOUTIAO_URL, wait_until="networkidle", timeout=45000)
            except Exception as timeout_err:
                print(f"[SYNC] âš  networkidle è¶…æ—¶ï¼Œé™çº§ç­‰å¾… DOM: {timeout_err}")
                await page.goto(TOUTIAO_URL, wait_until="domcontentloaded", timeout=30000)
            
            # åˆšåŠ è½½å®Œï¼Œéšæœºå‘å‘†å‡ ç§’ï¼Œæ¨¡æ‹Ÿäººçœ¼æµè§ˆ
            await human_delay(4, 6)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç 
            if await check_captcha(page, f"sync_try_{attempt}"):
                print(f"[SYNC] ç¬¬ {attempt} æ¬¡é­é‡éªŒè¯ç ï¼Œå…³é—­é‡è¯•...")
                raise Exception("Captcha detected")

            # ============================================
            # 2. å‡†å¤‡æ»šåŠ¨
            # ============================================
            articles_found = False
            links = []
            all_seen_urls = set()
            
            # ç­‰å¾…æ–‡ç« åˆ—è¡¨å®¹å™¨å‡ºç° (å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨)
            print("[SYNC] ç­‰å¾…æ–‡ç« åˆ—è¡¨åŠ è½½...")
            article_selectors = [
                'a[href*="/article/"]',
                'a[href*="/w/"]', 
                'a[href*="/video/"]',
                '.feed-card-wrapper', # å¤´æ¡å¸¸è§çš„å¡ç‰‡å®¹å™¨
                '.profile-feed-card'
            ]
            
            element_found = False
            for sel in article_selectors:
                try:
                    await page.wait_for_selector(sel, timeout=5000)
                    print(f"[SYNC] âœ“ æ£€æµ‹åˆ°å†…å®¹å…ƒç´ : {sel}")
                    element_found = True
                    break
                except:
                    continue
            
            if not element_found:
                print("[SYNC] âš  æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ–‡ç« å…ƒç´ ï¼Œå¯èƒ½åŠ è½½æ…¢ï¼Œå°è¯•ç›´æ¥æ»šåŠ¨...")

            # ============================================
            # 3. å¾ªç¯æ»šåŠ¨ (æ ¸å¿ƒé€»è¾‘)
            # ============================================
            print(f"[SYNC] å¼€å§‹æ»šåŠ¨åŠ è½½... (è®¡åˆ’æœ€å¤§æ»šåŠ¨: {MAX_SCROLL_ROUNDS_DYNAMIC} æ¬¡)")
            
            no_new_count = 0 # è¿ç»­æœªå‘ç°æ–°å†…å®¹çš„æ¬¡æ•°
            
            for scroll_round in range(MAX_SCROLL_ROUNDS_DYNAMIC):
                # --- A. æ‰§è¡Œæ»šåŠ¨åŠ¨ä½œ ---
                # éšæœºè·ç¦»ï¼Œæ¨¡æ‹ŸçœŸäººæ»šè½®
                scroll_distance = random.randint(500, 900) 
                await page.mouse.wheel(0, scroll_distance)
                
                # æ»šåŠ¨åçš„åœé¡¿ï¼Œç­‰å¾… AJAX åŠ è½½ (æ·±åº¦æŠ“å–æ—¶ç¨å¾®å¿«ä¸€ç‚¹ç‚¹ï¼Œä½†ä¸èƒ½å¤ªå¿«)
                wait_time = random.uniform(1.2, 2.0) if IS_COLD_START else random.uniform(1.5, 3.0)
                await asyncio.sleep(wait_time)
                
                # 10% æ¦‚ç‡å¾€å›æ»šä¸€ç‚¹ï¼Œéå¸¸åƒçœŸäººæ‰¾å›åˆšæ‰çœ‹è¿‡çš„æ ‡é¢˜
                if random.random() < 0.1:
                    await page.mouse.wheel(0, -random.randint(100, 300))
                    await asyncio.sleep(0.5)

                # --- B. æå–æ•°æ® (æ¯æ»š 2 æ¬¡æå–ä¸€æ¬¡ï¼Œå‡å°‘ JS æ³¨å…¥é¢‘ç‡) ---
                # ç¬¬ä¸€è½®å¿…é¡»æå–ï¼Œåç»­éš”è½®æå–ï¼Œæˆ–è€…æœ€åä¸€è½®å¼ºåˆ¶æå–
                if scroll_round == 0 or (scroll_round + 1) % 2 == 0 or scroll_round == MAX_SCROLL_ROUNDS_DYNAMIC - 1:
                    current_links = await page.evaluate(EXTRACT_LINKS_JS)
                    
                    # ç»Ÿè®¡æœ¬è½®æ–°å¢
                    # æ³¨æ„ï¼šcurrent_links åŒ…å«é¡µé¢ä¸Šæ‰€æœ‰ current DOM é‡Œçš„é“¾æ¥
                    #æˆ‘ä»¬éœ€è¦çœ‹å…¶ä¸­æœ‰å¤šå°‘æ˜¯ä¹‹å‰æ²¡è§è¿‡çš„
                    current_new_items = [l for l in current_links if l['href'] not in all_seen_urls]
                    
                    # æ›´æ–°å…¨å±€è®°å½•
                    for l in current_new_items:
                        all_seen_urls.add(l['href'])
                    
                    # æ›´æ–°å½“å‰æŒæœ‰çš„æ€»é“¾æ¥åˆ—è¡¨
                    links = current_links # ä¿å­˜æœ€æ–°çš„å…¨é‡å¿«ç…§
                    
                    total_grabbed = len(all_seen_urls)
                    print(f"[SYNC] æ»šåŠ¨ {scroll_round + 1}/{MAX_SCROLL_ROUNDS_DYNAMIC}: "
                          f"ç´¯è®¡å‘ç° {total_grabbed} ç¯‡ (æœ¬è½®æ–°å¢ {len(current_new_items)})")
                    
                    # --- C. åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢ ---
                    
                    # 1. è¿ç»­æ— æ–°å†…å®¹åˆ¤æ–­ (åˆ°åº•äº†)
                    if len(current_new_items) == 0:
                        no_new_count += 1
                        if no_new_count >= 4: # è¿ç»­ 4 æ¬¡æ²¡åˆ·å‡ºæ–°ä¸œè¥¿
                            print("[SYNC] ğŸ›‘ è¿ç»­ 4 æ¬¡æ»šåŠ¨æ— æ–°å†…å®¹ï¼Œåˆ¤æ–­å·²åˆ°åº•éƒ¨ï¼Œåœæ­¢ã€‚")
                            break
                    else:
                        no_new_count = 0 # é‡ç½®è®¡æ•°å™¨

                    # 2. æ•°é‡è¾¾æ ‡åˆ¤æ–­ (æå‰ç»“æŸ)
                    if total_grabbed >= TARGET_ARTICLE_COUNT:
                        print(f"[SYNC] ğŸ›‘ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ ({total_grabbed}/{TARGET_ARTICLE_COUNT})ï¼Œæå‰ç»“æŸã€‚")
                        break

            # ============================================
            # 4. ç»“æœå¤„ç†
            # ============================================
            if len(all_seen_urls) > 0:
                articles_found = True
                final_count = len(all_seen_urls)
                
                # æ•´ç†æœ€ç»ˆç»“æœ (å»é‡é€»è¾‘å…¶å®åœ¨ all_seen_urls å·²ç»åšäº†ä¸€éƒ¨åˆ†ï¼Œè¿™é‡Œç¡®ä¿æ ¼å¼)
                # EXTRACT_LINKS_JS è¿”å›çš„æ˜¯ listï¼Œæˆ‘ä»¬ç”¨æœ€åä¸€æ¬¡æŠ“å–çš„å¿«ç…§ï¼Œæˆ–è€…åˆå¹¶æ‰€æœ‰å†å²
                # è¿™é‡Œçš„ links å˜é‡å­˜çš„æ˜¯æœ€åä¸€æ¬¡ evaluate çš„ç»“æœï¼Œ
                # ä½†ä¸ºäº†ä¿é™©ï¼ˆé˜²æ­¢é¡µé¢å¤ªé•¿ DOM èŠ‚ç‚¹è¢«ç§»é™¤ï¼‰ï¼Œæˆ‘ä»¬æœ€å¥½é‡æ–°æ•´ç†ä¸€é
                # ç®€å•èµ·è§ï¼Œç›´æ¥ä½¿ç”¨ links (é€šå¸¸åŒ…å«é¡µé¢å¤§éƒ¨åˆ†å†…å®¹)
                # å¦‚æœæ˜¯æ— é™æ»šåŠ¨ä¸” DOM èŠ‚ç‚¹å›æ”¶çš„ç½‘é¡µï¼Œéœ€è¦ç”¨ all_seen_urls é…åˆå†å² item å­˜å‚¨
                # é‰´äºå¤´æ¡ PC ç«¯é€šå¸¸ä¿ç•™èŠ‚ç‚¹ï¼Œç›´æ¥ç”¨ links å³å¯ï¼Œæˆ–è€…ç”¨ evaluate å†è·‘ä¸€æ¬¡
                
                final_links = await page.evaluate(EXTRACT_LINKS_JS)
                print(f"\n[SYNC] âœ… åŒæ­¥æˆåŠŸ! ç¬¬ {attempt} æ¬¡å°è¯•ï¼Œé¡µé¢å…±å­˜åœ¨ {len(final_links)} ç¯‡æ–‡ç« ")
                
                # æ‰“å°æ ·æœ¬
                print("[SYNC] æ–‡ç« æ ·æœ¬:")
                for i, link in enumerate(final_links[:3], 1):
                    print(f"       {i}. {link['text'][:30]}...")

                # å…¥åº“
                db.add_articles(final_links)
                db.mark_synced()
                
                # æ¸…ç†å·¥ä½œ
                try:
                    # æˆåŠŸåæˆªå›¾ç•™åº•
                    await page.screenshot(path=DEBUG_DIR / "sync_success_latest.png")
                    # æ¸…ç†æ—§çš„æŠ¥é”™å›¾
                    for p_file in DEBUG_DIR.glob("error_sync_*.png"): p_file.unlink(missing_ok=True)
                except: pass
                
                await page.close()
                return # æˆåŠŸé€€å‡ºå‡½æ•°

            else:
                # æ²¡æŠ“åˆ°
                print(f"[WARN] ç¬¬ {attempt} æ¬¡å°è¯•æ»šåŠ¨åæœªå‘ç°æ–‡ç«  (å¯èƒ½æ˜¯åŠ è½½å¤±è´¥æˆ–ç©ºç™½é¡µ)")
                await page.screenshot(path=DEBUG_DIR / f"error_sync_empty_{attempt}.png")
                
                if attempt < MAX_RETRIES:
                    print("[SYNC] å°è¯•åˆ·æ–°é¡µé¢é‡è¯•...")
                    await asyncio.sleep(3)

        except Exception as e:
            print(f"[SYNC] âŒ ç¬¬ {attempt} æ¬¡å°è¯•å‘ç”Ÿå¼‚å¸¸: {e}")
            try:
                if not page.is_closed():
                    await page.screenshot(path=DEBUG_DIR / f"error_sync_exception_{attempt}.png")
            except: pass
            
            # å¤±è´¥åå†·å´
            await asyncio.sleep(random.randint(5, 10))
        
        finally:
            try:
                if not page.is_closed(): await page.close()
            except: pass

    print("[SYNC] âŒ å…¨é‡åŒæ­¥ä»»åŠ¡æœ€ç»ˆå¤±è´¥ (æ‰€æœ‰é‡è¯•è€—å°½)")


async def read_article_task(context: BrowserContext, article: dict, db: ArticleDB):
    """
    å•ç¯‡é˜…è¯»ä»»åŠ¡ - ä¼˜åŒ–ç‰ˆ
    æ”¹åŠ¨ï¼š
    1. çƒ­èº«å·²åœ¨ main() å®Œæˆï¼Œè¿™é‡Œç›´æ¥è®¿é—®æ–‡ç« 
    2. æ¯ç¯‡æ–‡ç« æˆåŠŸåæˆªå›¾
    3. æˆåŠŸåæ¸…ç†ä¸Šä¸€æ¬¡çš„é”™è¯¯æˆªå›¾
    4. ä¿ç•™æ‰€æœ‰æ‹ŸäººåŒ–æ“ä½œ
    """
    url = article['url']
    title_preview = article['title'][:30]
    print(f"--- [READ] æ­£åœ¨æ‰“å¼€: {title_preview}... ---")
    
    page = await context.new_page()
    
    # âš ï¸ æš‚æ—¶ç¦ç”¨ stealthï¼ˆæµ‹è¯•è¯æ˜å¯èƒ½æœ‰è´Ÿé¢å½±å“ï¼‰
    # if HAS_STEALTH: await stealth_async(page)

    try:
        # ============================================
        # ğŸ”¥ ç›´æ¥è®¿é—®æ–‡ç« é¡µï¼Œçƒ­èº«å·²åœ¨ main() å®Œæˆ
        # ============================================
        await page.goto(url, wait_until="domcontentloaded", timeout=45000)
        
        # 1. éªŒè¯ç ä¸404æ£€æŸ¥
        await human_delay(2, 3)
        if await check_captcha(page, "read"):
            return

        page_content = await page.evaluate("document.body.innerText")
        page_title = await page.title()
        
        # ç®€æ˜“çš„å¤±æ•ˆåˆ¤æ–­
        invalid_keywords = ["404", "é¡µé¢ä¸å­˜åœ¨", "æ–‡ç« å·²åˆ é™¤", "å‚æ•°é”™è¯¯"]
        if any(k in page_title for k in invalid_keywords):
            print("[READ] æ–‡ç« å·²å¤±æ•ˆï¼Œæ ‡è®° invalidã€‚")
            db.mark_invalid(url)
            return

        # =========================================================
        # é˜…è¯»æ—¶é•¿è®¡ç®—ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
        # =========================================================
        
        # 1. å­—æ•°ç»Ÿè®¡
        word_count = len(page_content)
        
        # 2. å›¾ç‰‡æ•°é‡ç»Ÿè®¡
        img_count = await page.evaluate("""
            () => {
                const imgs = document.querySelectorAll('article img, .tt-input__content img, .article-content img, .pgc-img img');
                return imgs.length;
            }
        """)

        # 3. è®¡ç®—åŸºå‡†æ—¶é•¿
        text_time = word_count / 25.0  
        img_time = img_count * 5.0
        base_time = text_time + img_time
        
        if base_time < 10:
            base_time = random.randint(20, 40)
        
        # 4. å¢åŠ éšæœºæ‰°åŠ¨
        variation = random.gauss(1.0, 0.2)
        thinking_time = random.uniform(5, 15)
        
        # è®¡ç®—æ€»æ—¶é•¿
        calc_seconds = (base_time * variation) + thinking_time
        
        # 5. ä¸¥æ ¼æˆªæ–­ (30s ~ 180s)
        read_seconds = max(30.0, calc_seconds)
        read_seconds = min(180.0, read_seconds)
        
        print(f"[READ] å­—æ•°:{word_count} | å›¾ç‰‡:{img_count} | ç®—æ³•è®¡ç®—:{calc_seconds:.1f}s")
        print(f"[READ] >> æœ€ç»ˆè®¡åˆ’åœç•™: {read_seconds:.1f}ç§’")
        
        # =========================================================
        # æ‹ŸäººåŒ–äº¤äº’å¾ªç¯ï¼ˆå®Œæ•´ä¿ç•™ï¼‰
        # =========================================================
        start_read = time.time()
        scroll_count = 0
        
        while (time.time() - start_read) < read_seconds:
            # éšæœºä¸‹æ»‘
            await human_scroll(page, max_scrolls=1)
            scroll_count += 1
            
            # éšæœºé¼ æ ‡ç§»åŠ¨
            if random.random() < 0.3:
                await human_mouse_move(
                    page, 
                    random.randint(200, 1000), 
                    random.randint(300, 800)
                )
            
            # æä½æ¦‚ç‡æ¨¡æ‹Ÿé€‰ä¸­æ–‡æœ¬
            if random.random() < 0.1:
                try:
                    await page.click("p", timeout=200)
                except: 
                    pass
            
            # æä½æ¦‚ç‡çŸ­æš‚åœé¡¿ï¼ˆæ¨¡æ‹Ÿæ€è€ƒï¼‰
            if random.random() < 0.05:
                await asyncio.sleep(random.uniform(2, 5))

        # å¿…é¡»åŠ¨ä½œï¼šæ»‘åŠ¨åˆ°åº•éƒ¨
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await human_delay(1.5, 3.0)
        
        # ============================================
        # ğŸ”¥ æˆåŠŸï¼šæˆªå›¾ + æ¸…ç†æ—§é”™è¯¯æ–‡ä»¶
        # ============================================
        print(f"[READ] âœ… é˜…è¯»å®Œæˆ (æ»šåŠ¨{scroll_count}æ¬¡)")
        
        # ç”Ÿæˆæˆªå›¾æ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³åŒºåˆ†ï¼‰
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ä»URLæå–æ–‡ç« IDä½œä¸ºæ ‡è¯†
        article_id = url.split('/')[-1][:12] if '/' in url else "unknown"
        screenshot_name = f"read_success_{timestamp_str}_{article_id}.png"
        
        try:
            await page.screenshot(path=DEBUG_DIR / screenshot_name)
            print(f"[READ] ğŸ“¸ å·²ä¿å­˜æˆªå›¾: {screenshot_name}")
        except Exception as ss_err:
            print(f"[READ] âš  æˆªå›¾å¤±è´¥: {ss_err}")
        
        # æ¸…ç†æ—§çš„é”™è¯¯æˆªå›¾
        try:
            for file_path in DEBUG_DIR.glob("error_read_*.png"):
                file_path.unlink(missing_ok=True)
            # åªä¿ç•™æœ€è¿‘5å¼ æˆåŠŸæˆªå›¾ï¼Œåˆ é™¤æ›´æ—©çš„
            success_screenshots = sorted(
                DEBUG_DIR.glob("read_success_*.png"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            for old_file in success_screenshots[5:]:
                old_file.unlink(missing_ok=True)
        except Exception as clean_err:
            print(f"[READ] âš  æ¸…ç†æ—§æˆªå›¾å¤±è´¥: {clean_err}")
        
        # è®°å½•é˜…è¯»
        db.record_read(url)

    except Exception as e:
        print(f"[READ] âŒ å¼‚å¸¸: {e}")
        # å‡ºé”™æ—¶æˆªå›¾
        try:
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            await page.screenshot(path=DEBUG_DIR / f"error_read_{timestamp_str}.png")
            print(f"[READ] å·²ä¿å­˜é”™è¯¯æˆªå›¾")
        except:
            pass
    
    finally:
        await page.close()


# ================= ä¸»ç¨‹åºå…¥å£ =================

async def main():
    # 1. å‡†å¤‡å·¥ä½œ
    db = ArticleDB(DB_FILE)
    
    # éšæœºé€‰æ‹©è§†çª—
    vp = random.choice(VIEWPORTS)
    # è·å–éšæœº UA
    ua = get_pc_user_agent()
    
    print(f"[INIT] å¯åŠ¨çˆ¬è™«ä»»åŠ¡")
    print(f"[INIT] UA: {ua[:50]}...")
    print(f"[INIT] Viewport: {vp['width']}x{vp['height']}")

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-infobars",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                f"--window-size={vp['width']},{vp['height']}"
            ]
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = await browser.new_context(
            user_agent=ua,
            viewport=vp,
            locale="zh-CN",
            timezone_id="Asia/Shanghai",
            device_scale_factor=1,
            has_touch=False,
            is_mobile=False,
            java_script_enabled=True
        )

        # æ³¨å…¥ webdriver ç§»é™¤è„šæœ¬
        await context.add_init_script(
            "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        )

        # ============================================
        # ğŸ”¥ ç»Ÿä¸€çƒ­èº«ï¼šåœ¨æ‰€æœ‰ä»»åŠ¡ä¹‹å‰ï¼Œç”¨ç‹¬ç«‹ Page
        # ============================================
        print("[WARMUP] æ‰§è¡Œä¸€æ¬¡æ€§çƒ­èº«...")
        warmup_page = None
        try:
            warmup_page = await context.new_page()
            await warmup_page.goto(
                "https://www.toutiao.com/", 
                wait_until="domcontentloaded",  # ä¸ç”¨ networkidle
                timeout=30000
            )
            await human_delay(2, 4)
            
            # ç®€å•äº¤äº’
            await human_mouse_move(warmup_page, 500, 400)
            await warmup_page.mouse.wheel(0, random.randint(200, 400))
            await human_delay(1, 2)
            
            print("[WARMUP] âœ“ çƒ­èº«å®Œæˆ")
        except Exception as e:
            print(f"[WARMUP] âš  çƒ­èº«å¤±è´¥(å¯å¿½ç•¥): {e}")
        finally:
            if warmup_page:
                try:
                    await warmup_page.close()
                except:
                    pass
        
        # çƒ­èº«åçŸ­æš‚ç­‰å¾…
        await asyncio.sleep(random.uniform(1, 2))

        # ============================================
        # æ­¥éª¤ 1: æ£€æŸ¥æ˜¯å¦éœ€è¦å…¨é‡åŒæ­¥
        # ============================================
        if db.needs_sync() or not db.data.get("articles"):
            print("\n[TASK] å¼€å§‹åŒæ­¥ä»»åŠ¡...")
            await sync_task(context, db)
        else:
            print("[INIT] ä»Šæ—¥å·²æ‰§è¡Œè¿‡åŒæ­¥ï¼Œè·³è¿‡åˆ—è¡¨æŠ“å–ã€‚")

        # ============================================
        # æ­¥éª¤ 2: è·å–ä»Šæ—¥é˜…è¯»ç›®æ ‡
        # ============================================
        targets = db.get_weighted_candidates()
        
        if not targets:
            print("[DONE] æš‚æ— å¾…è¯»æ–‡ç«  (å¯èƒ½å·²å…¨éƒ¨è¯»å®Œæˆ–æ— æ–°å†…å®¹)ã€‚")
            await browser.close()
            return

        print(f"\n[TASK] ä»Šæ—¥é˜…è¯»è®¡åˆ’: {len(targets)} ç¯‡æ–‡ç« ")

        # ============================================
        # æ­¥éª¤ 3: å¾ªç¯é˜…è¯»
        # æ³¨æ„ï¼šçƒ­èº«å·²å®Œæˆï¼Œæ¯ç¯‡æ–‡ç« ç›´æ¥è®¿é—®
        # ============================================
        for i, article in enumerate(targets, 1):
            print(f"\n{'='*50}")
            print(f">>> è¿›åº¦ [{i}/{len(targets)}]")
            print(f"{'='*50}")
            
            await read_article_task(context, article, db)
            
            # ç¯‡é—´å†·å´æ—¶é—´
            if i < len(targets):
                wait_time = random.randint(8, 15)
                print(f"[COOL] ä¼‘æ¯ {wait_time} ç§’...")
                await asyncio.sleep(wait_time)

        # ============================================
        # å®Œæˆ
        # ============================================
        await browser.close()
        print("\n" + "="*50)
        print("[DONE] âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        print("="*50)


if __name__ == "__main__":
    asyncio.run(main())


