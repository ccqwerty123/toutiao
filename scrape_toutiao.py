import asyncio
import json
import random
import time
import math
import sys
from datetime import datetime
from pathlib import Path

from playwright.async_api import async_playwright, Page, BrowserContext

# ================= ä¾èµ–åº“æ£€æµ‹ =================
try:
    # å°è¯•å¯¼å…¥ playwright-stealth å¢å¼ºé˜²çˆ¬èƒ½åŠ›
    from playwright_stealth import stealth_async
    HAS_STEALTH = True
except ImportError:
    HAS_STEALTH = False
    print("================================================================")
    print(f"[WARN] æœªå®‰è£… playwright-stealth åº“ã€‚")
    print(f"[WARN] å»ºè®®è¿è¡Œ: pip install playwright-stealth ä»¥é™ä½è¢«æ£€æµ‹é£é™©ã€‚")
    print("================================================================")

# ================= é…ç½®åŒºåŸŸ =================

# ç›®æ ‡ç”¨æˆ·ä¸»é¡µ Token URL (è¯·ç¡®ä¿æ­¤é“¾æ¥æœ‰æ•ˆ)
TOUTIAO_URL = "https://www.toutiao.com/c/user/token/CiyRLPHkUyTCD9FmHodOGQVcmZh5-NRKyfiTSF0XMms-tSja0FdhrUWRp-T-DBpJCjwAAAAAAAAAAAAAT8lExjCbDHcWTgszQQjqU0Ohh9qtuXbuEOe6CQdqJEZ7yIpoM-NJ93_Sty1iMpOe_FUQ9ZmDDhjDxYPqBCIBA9GPpzc="

# è¾“å‡ºè®¾ç½®
DATA_DIR = Path("data")
DATA_DIR.mkdir(parents=True, exist_ok=True)

DB_FILE = DATA_DIR / "toutiao_db.json"
DEBUG_DIR = DATA_DIR / "debug"
DEBUG_DIR.mkdir(parents=True, exist_ok=True)

# æµè§ˆè¡Œä¸ºé™åˆ¶
MAX_READ_COUNT = 50     # æ¯æ¬¡è¿è¡Œè„šæœ¬æœ€å¤šé˜…è¯»å¤šå°‘ç¯‡
MIN_READ_COUNT = 7      # æ¯æ¬¡è¿è¡Œè„šæœ¬æœ€å°‘é˜…è¯»å¤šå°‘ç¯‡
MAX_SYNC_SCROLLS = 20   # åŒæ­¥åˆ—è¡¨æ—¶æœ€å¤§ä¸‹æ»‘æ¬¡æ•°
AGING_THRESHOLD = 50    # æ–‡ç« â€œè€åŒ–â€é˜ˆå€¼
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°


# ================= User-Agent ç®¡ç† =================

# å†…ç½®å…œåº• PC UA åº“ (è¦†ç›–ä¸»æµæµè§ˆå™¨ä¸æ“ä½œç³»ç»Ÿ)
FALLBACK_PC_UAS = [
    # Windows 10/11 Chrome
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    # Windows Edge
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
    # Mac Chrome
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, Like Gecko) Chrome/119.0.0.0 Safari/537.36",
    # Mac Safari
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    # Linux Chrome
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

def get_pc_user_agent():
    """
    ä¼˜å…ˆä½¿ç”¨ real-useragent åº“è·å–éšæœº PC UAã€‚
    å¦‚æœè·å–å¤±è´¥æˆ–åº“æœªå®‰è£…ï¼Œä½¿ç”¨å†…ç½®åˆ—è¡¨å…œåº•ã€‚
    """
    ua = ""
    try:
        from real_useragent import UserAgent
        rua = UserAgent()
        ua = rua.desktop_useragent()
        # ç®€å•æ ¡éªŒè·å–çš„UAæ˜¯å¦åˆæ³•
        if not ua or len(ua) < 20:
            raise ValueError("UA too short")
    except Exception:
        ua = random.choice(FALLBACK_PC_UAS)
    
    return ua

# å¸¸è§ PC åˆ†è¾¨ç‡åº“ (é¿å…å•ä¸€æŒ‡çº¹)
VIEWPORTS = [
    {"width": 1920, "height": 1080},
    {"width": 1536, "height": 864},
    {"width": 1440, "height": 900},
    {"width": 1366, "height": 768},
    {"width": 1280, "height": 720},
]

# ================= JS æ³¨å…¥è„šæœ¬ (æ ¸å¿ƒé€»è¾‘ä¼˜åŒ–) =================
# å¢å¼ºç‰ˆé“¾æ¥æå–è„šæœ¬ï¼šå…¼å®¹æ€§ä¿®å¤(ç§»é™¤?.) + å¢å¼ºæ ‡é¢˜æå– + è¿‡æ»¤è¯æ›´æ–°
EXTRACT_LINKS_JS = r"""
() => {
  const anchors = Array.from(document.querySelectorAll("a[href]"));
  const origin = window.location.origin;
  const results = [];
  const seen = new Set();
  
  // 1. è·¯å¾„ç‰¹å¾åˆ¤æ–­
  const isArticle = (path) => {
    if (!path) return false;
    if (path.startsWith("/c/user/")) return false;
    if (path.startsWith("/search/")) return false;
    if (path.includes("toutiao_search")) return false;

    const lastPart = path.split("/").filter(Boolean).pop();
    if (!lastPart) return false;
    const digits = lastPart.replace(/\D/g, "").length;
    return digits > 5;
  };

  // 2. åŸºç¡€æ–‡æœ¬æå–
  const getText = (el) => {
    if (!el) return "";
    let txt = (el.innerText || "").trim();
    if (txt) return txt;
    
    txt = (el.getAttribute("aria-label") || "").trim();
    if (txt) return txt;
    
    txt = (el.getAttribute("title") || "").trim();
    if (txt) return txt;
    
    const img = el.querySelector("img");
    if (img) {
        txt = (img.getAttribute("alt") || "").trim();
    }
    return txt;
  };

  // 3. æˆªå–æ–‡æœ¬
  const truncateText = (text, maxLength = 50) => {
    if (!text || text.length <= maxLength) return text;
    return text.substring(0, maxLength) + "...";
  };

  // 4. è·å–å†…å®¹ç±»å‹
  const getContentType = (url) => {
    if (url.includes("/article/")) return "article";
    if (url.includes("/w/")) return "weitoutiao";
    if (url.includes("/video/")) return "video";
    return "unknown";
  };

  // 5. å¢å¼ºçš„æ ‡é¢˜æå–
  const extractTitle = (a, urlObj) => {
    let text = getText(a);
    const contentType = getContentType(urlObj.pathname);

    // å¦‚æœç›´æ¥è·å–å¤±è´¥æˆ–æ–‡æœ¬å¤ªçŸ­ï¼Œå°è¯•æ›´å¤šæ–¹æ³•
    if (!text || text.length < 4) {
        let container = a.closest('.feed-card-wrapper, .article-card, .feed-card-article-wrapper, .card-wrapper, .weitoutiao-wrap, .wtt-content');
        
        // --- å…¼å®¹æ€§ä¿®æ”¹ï¼šä¸ä½¿ç”¨ ?. æ“ä½œç¬¦ ---
        if (!container) {
            if (a.parentElement && a.parentElement.parentElement && a.parentElement.parentElement.parentElement) {
                container = a.parentElement.parentElement.parentElement;
            }
        }

        if (container) {
            // å¾®å¤´æ¡ç­–ç•¥
            if (contentType === "weitoutiao") {
                const contentEl = container.querySelector('.weitoutiao-content, .wtt-content, .feed-card-article-content, [class*="content"]');
                if (contentEl) {
                    const content = contentEl.innerText.trim();
                    if (content) {
                        text = truncateText(content, 40);
                    }
                }
            } 
            // è§†é¢‘ç­–ç•¥
            else if (contentType === "video") {
                const titleEl = container.querySelector('.video-title, .title, [class*="title"]');
                if (titleEl) {
                    const t = titleEl.innerText.trim();
                    if (t) text = t;
                }
            }

            // é€šç”¨æ ‡é¢˜æŸ¥æ‰¾
            if (!text || text.length < 4) {
                const selectors = [
                    '.title', '.feed-card-article-title', '.article-title', '.feed-card-article-l a',
                    '[class*="title"]', 'h1, h2, h3', '.text', 'p'
                ];

                for (const selector of selectors) {
                    const el = container.querySelector(selector);
                    if (el) {
                        const t = el.innerText.trim();
                        if (t && t.length > 4) {
                            text = truncateText(t, 50);
                            break;
                        }
                    }
                }
            }

            // æœ€åå°è¯•ï¼šè·å–å®¹å™¨å†…ç¬¬ä¸€ä¸ªé•¿æ–‡æœ¬
            if (!text || text.length < 4) {
                const allTexts = container.innerText.trim().split('\n').filter(t => t.trim().length > 4);
                if (allTexts.length > 0) {
                    text = truncateText(allTexts[0], 50);
                }
            }
        }
    }

    // å…œåº•é‡å‘½å
    if (!text || text === "Untitled") {
        if (contentType === "weitoutiao") text = "[å¾®å¤´æ¡]";
        else if (contentType === "video") text = "[è§†é¢‘]";
    }

    return { text: text || "Untitled", contentType };
  };

  // ä¸»å¾ªç¯
  for (const a of anchors) {
    let href = a.getAttribute("href");
    if (!href) continue;
    
    if (href.startsWith("/")) href = origin + href;
    
    try {
        const urlObj = new URL(href);
        
        if (!urlObj.hostname.includes("toutiao.com")) continue;
        if (!isArticle(urlObj.pathname)) continue;
        
        const cleanUrl = urlObj.origin + urlObj.pathname;
        if (seen.has(cleanUrl)) continue;

        const titleInfo = extractTitle(a, urlObj);
        let text = titleInfo.text;

        // å…³é”®è¯è¿‡æ»¤ (å·²æ·»åŠ 'ä¾µæƒä¸¾æŠ¥å—ç†å…¬ç¤º')
        const filterKeywords = [
            'è·Ÿå¸–è¯„è®ºè‡ªå¾‹ç®¡ç†æ‰¿è¯ºä¹¦',
            'ç”¨æˆ·åè®®',
            'éšç§æ”¿ç­–',
            'ä¾µæƒæŠ•è¯‰',
            'ç½‘ç»œè°£è¨€æ›å…‰å°',
            'è¿æ³•å’Œä¸è‰¯ä¿¡æ¯ä¸¾æŠ¥',
            'ä¾µæƒä¸¾æŠ¥å—ç†å…¬ç¤º'
        ];
        
        if (filterKeywords.some(keyword => text.includes(keyword))) continue;

        // é¢å¤–çš„çŸ­è¯è¿‡æ»¤
        if (!text.startsWith('[') && text.match(/^(å¤‡æ¡ˆ|ä¸¾æŠ¥|ç™»å½•|ä¸‹è½½|å¹¿å‘Š|ç›¸å…³æ¨è|æœç´¢)$/)) continue;
        
        seen.add(cleanUrl);
        results.push({ 
            text: text, 
            href: cleanUrl,
            type: titleInfo.contentType
        });

    } catch(e) {}
  }

  return results;
}
"""

# ================= æ•°æ®åº“ç®¡ç†ç±» =================

class ArticleDB:
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.data = self._load()

    def _load(self):
        if not self.db_path.exists():
            return {"last_sync_date": "", "articles": {}}
        try:
            return json.loads(self.db_path.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[DB] è¯»å–æ•°æ®åº“å‡ºé”™: {e}ï¼Œå°†åˆå§‹åŒ–æ–°åº“")
            return {"last_sync_date": "", "articles": {}}

    def save(self):
        try:
            self.db_path.write_text(json.dumps(self.data, indent=2, ensure_ascii=False), encoding="utf-8")
        except Exception as e:
            print(f"[DB] ä¿å­˜å¤±è´¥: {e}")

    def needs_sync(self) -> bool:
        """åˆ¤æ–­ä»Šå¤©æ˜¯å¦å·²ç»æ‰§è¡Œè¿‡å…¨é‡æŠ“å–"""
        today = datetime.now().strftime("%Y-%m-%d")
        return self.data.get("last_sync_date") != today

    def mark_synced(self):
        self.data["last_sync_date"] = datetime.now().strftime("%Y-%m-%d")
        self.save()

    def add_articles(self, scraped_items: list):
        """å¢é‡æ·»åŠ æ–‡ç« """
        added_count = 0
        current_urls = self.data["articles"]
        
        for item in scraped_items:
            url = item['href']
            # å¦‚æœæ˜¯æ–°é“¾æ¥ï¼Œæˆ–è€…æ—§é“¾æ¥æ˜¯Untitledä½†è¿™æ¬¡æŠ“åˆ°äº†çœŸæ ‡é¢˜ï¼Œåˆ™æ›´æ–°
            if url not in current_urls:
                current_urls[url] = {
                    "title": item['text'],
                    "url": url,
                    "status": "active",
                    "last_read_at": "",
                    "read_count": 0
                }
                added_count += 1
            elif current_urls[url]["title"] == "Untitled" and item['text'] != "Untitled":
                 current_urls[url]["title"] = item['text'] # ä¿®æ­£æ ‡é¢˜
        
        print(f"[DB] æ•°æ®åº“æ›´æ–°: æ–°å¢ {added_count} ç¯‡ï¼Œå½“å‰æ€»åº“å­˜ {len(current_urls)} ç¯‡")
        self.save()

    def mark_invalid(self, url):
        """æ ‡è®°å¤±æ•ˆ"""
        if url in self.data["articles"]:
            self.data["articles"][url]["status"] = "invalid"
            print(f"[DB] é“¾æ¥æ ‡è®°ä¸ºæ— æ•ˆ: {url}")
            self.save()

    def record_read(self, url):
        """è®°å½•é˜…è¯»"""
        if url in self.data["articles"]:
            today = datetime.now().strftime("%Y-%m-%d")
            entry = self.data["articles"][url]
            entry["last_read_at"] = today
            entry["read_count"] = entry.get("read_count", 0) + 1
            self.save()

    def get_weighted_candidates(self) -> list:
        """è·å–ä»Šæ—¥é˜…è¯»åˆ—è¡¨ï¼šæƒé‡ç®—æ³•"""
        today = datetime.now().strftime("%Y-%m-%d")
        candidates = []
        weights = []
        
        active_urls = [k for k, v in self.data["articles"].items() if v.get("status") == "active"]
        
        for url in active_urls:
            info = self.data["articles"][url]
            
            # è§„åˆ™1: ä»Šå¤©è¯»è¿‡çš„ç»å¯¹ä¸è¯»
            if info.get("last_read_at") == today:
                continue
            
            read_count = info.get("read_count", 0)
            
            # è§„åˆ™2: æƒé‡è®¡ç®—
            # æ²¡è¯»è¿‡çš„(0æ¬¡): æé«˜æƒé‡ 200
            # è¯»å¾—å°‘çš„(<5æ¬¡): é«˜æƒé‡ 100
            # æ™®é€š(<20æ¬¡): ä¸­æƒé‡ 50
            # è€æ—§(>50æ¬¡): ä½æƒé‡ 5 (ä¿ç•™å¾®å°æ¦‚ç‡)
            if read_count == 0:
                w = 200
            elif read_count < 5:
                w = 100
            elif read_count < 20:
                w = 50
            elif read_count < AGING_THRESHOLD:
                w = 20
            else:
                w = 5
            
            candidates.append(info)
            weights.append(w)
            
        if not candidates:
            return []

        # æ— æ”¾å›æŠ½å–
        target_k = random.randint(MIN_READ_COUNT, MAX_READ_COUNT)
        target_k = min(target_k, len(candidates))
        
        print(f"[PLAN] å¯é€‰æ–‡ç« åº“: {len(candidates)} ç¯‡. è®¡åˆ’é˜…è¯»: {target_k} ç¯‡")
        
        selected = []
        temp_cand = list(candidates)
        temp_weight = list(weights)
        
        for _ in range(target_k):
            if not temp_cand: break
            chosen = random.choices(temp_cand, weights=temp_weight, k=1)[0]
            selected.append(chosen)
            
            idx = temp_cand.index(chosen)
            temp_cand.pop(idx)
            temp_weight.pop(idx)
            
        return selected

# ================= æ‹ŸäººåŒ–æ“ä½œå‡½æ•° =================

async def human_delay(min_s=1.0, max_s=3.0):
    """å¸¦éšæœºæ€§çš„ç­‰å¾…"""
    await asyncio.sleep(random.uniform(min_s, max_s))

async def human_mouse_move(page: Page, x_target, y_target, steps=25):
    """è´å¡å°”æ›²çº¿æ¨¡æ‹Ÿé¼ æ ‡ç§»åŠ¨"""
    try:
        start_x = random.randint(100, 1000)
        start_y = random.randint(100, 600)
        
        ctrl_x1 = start_x + (x_target - start_x) * 0.3 + random.randint(-50, 50)
        ctrl_y1 = start_y + (y_target - start_y) * 0.3 + random.randint(-50, 50)
        ctrl_x2 = start_x + (x_target - start_x) * 0.7 + random.randint(-50, 50)
        ctrl_y2 = start_y + (y_target - start_y) * 0.7 + random.randint(-50, 50)

        for i in range(steps + 1):
            t = i / steps
            x = (1-t)**3 * start_x + 3*(1-t)**2 * t * ctrl_x1 + 3*(1-t)*t**2 * ctrl_x2 + t**3 * x_target
            y = (1-t)**3 * start_y + 3*(1-t)**2 * t * ctrl_y1 + 3*(1-t)*t**2 * ctrl_y2 + t**3 * y_target
            
            # æŠ–åŠ¨
            x += random.uniform(-2, 2)
            y += random.uniform(-2, 2)
            
            await page.mouse.move(x, y)
            await asyncio.sleep(random.uniform(0.005, 0.015))
    except Exception:
        pass

async def human_scroll(page: Page, max_scrolls=1):
    """æ‹ŸäººåŒ–æ»šåŠ¨"""
    for _ in range(max_scrolls):
        # éšæœºæ»šåŠ¨å¹…åº¦
        delta_y = random.randint(300, 700)
        await page.mouse.wheel(0, delta_y)
        
        # æ»šåŠ¨åçš„åœé¡¿ï¼Œæ¨¡æ‹Ÿé˜…è¯»
        await human_delay(1.0, 2.5)
        
        # 20% æ¦‚ç‡å›æ»š (å›çœ‹)
        if random.random() < 0.2:
            await page.mouse.wheel(0, -random.randint(100, 250))
            await human_delay(0.5, 1.2)

async def check_captcha(page: Page, tag="unknown") -> bool:
    """æ£€æŸ¥éªŒè¯ç ï¼Œå¹¶æˆªå›¾ï¼ˆè¦†ç›–æœ€æ–°ä¸€ä»½ï¼‰"""
    try:
        title = await page.title()
        is_captcha = False
        
        # 1. æ ‡é¢˜åˆ¤æ–­
        if any(kw in title for kw in ["éªŒè¯", "å®‰å…¨æ£€æµ‹", "captcha", "verify"]):
            is_captcha = True
            
        # 2. DOM åˆ¤æ–­
        if not is_captcha:
            if await page.query_selector("#captcha-verify-image") or \
               await page.query_selector(".captcha_verify_container"):
                is_captcha = True
        
        if is_captcha:
            print(f"[ALERT] {tag} é˜¶æ®µæ£€æµ‹åˆ°éªŒè¯ç ! Title: {title}")
            # ä¿å­˜éªŒè¯ç æˆªå›¾ï¼Œè¦†ç›–æ—§çš„åŒç±»å‹æ–‡ä»¶
            screenshot_path = DEBUG_DIR / f"captcha_{tag}_latest.png"
            await page.screenshot(path=screenshot_path)
            print(f"[ALERT] éªŒè¯ç æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
            return True
            
        return False
    except Exception as e:
        print(f"[WARN] éªŒè¯ç æ£€æµ‹å‡ºé”™: {e}")
        return False

# ================= æ ¸å¿ƒä»»åŠ¡é€»è¾‘ =================

async def sync_task(context: BrowserContext, db: ArticleDB):
    """
    å…¨é‡åŒæ­¥ä»»åŠ¡ - æ™ºèƒ½æ¨¡å¼ç‰ˆ
    
    ç­–ç•¥ï¼š
    1. é¦–æ¬¡è¿è¡Œ/åº“å­˜å°‘ â†’ å…¨é‡æ¨¡å¼ï¼šæ»‘åˆ°åº•ï¼Œå°½å¯èƒ½æŠ“å–æ‰€æœ‰æ–‡ç« 
    2. åº“å­˜å……è¶³ â†’ å¢é‡æ¨¡å¼ï¼šå¿«é€Ÿæ‰«æï¼Œè¦†ç›–æ›´æ–°å³å¯
    """
    print(">>> [SYNC] å¼€å§‹æ‰§è¡Œå…¨é‡åŒæ­¥ä»»åŠ¡...")
    
    # ============================================
    # ğŸ”¥ åˆ¤æ–­åŒæ­¥æ¨¡å¼
    # ============================================
    current_count = len(db.data.get("articles", {}))
    
    # é¦–æ¬¡è¿è¡Œæˆ–åº“å­˜å°‘äº100ç¯‡ â†’ å…¨é‡æ¨¡å¼
    FULL_SYNC_THRESHOLD = 100
    is_full_sync = current_count < FULL_SYNC_THRESHOLD
    
    if is_full_sync:
        print(f"[SYNC] ğŸ“¦ å…¨é‡æ¨¡å¼ï¼šå½“å‰åº“å­˜ {current_count} ç¯‡ < {FULL_SYNC_THRESHOLD}ï¼Œå°†å°½å¯èƒ½æŠ“å–æ‰€æœ‰æ–‡ç« ")
        max_scroll_rounds = 50      # æœ€å¤šæ»šåŠ¨50æ¬¡
        early_stop_count = 80       # ä¸æå‰åœæ­¢ï¼ˆè®¾å¾ˆå¤§ï¼‰
        no_new_threshold = 5        # è¿ç»­5æ¬¡æ— æ–°å†…å®¹æ‰åœ
    else:
        print(f"[SYNC] ğŸ”„ å¢é‡æ¨¡å¼ï¼šå½“å‰åº“å­˜ {current_count} ç¯‡ï¼Œå¿«é€Ÿæ›´æ–°å³å¯")
        max_scroll_rounds = 15      # æœ€å¤šæ»šåŠ¨15æ¬¡
        early_stop_count = 30       # è·å–30ç¯‡å³å¯åœæ­¢
        no_new_threshold = 3        # è¿ç»­3æ¬¡æ— æ–°å†…å®¹å°±åœ
    
    for attempt in range(1, MAX_RETRIES + 1):
        print(f">>> [SYNC] ç¬¬ {attempt}/{MAX_RETRIES} æ¬¡å°è¯•è¿æ¥...")
        page = await context.new_page()
        
        # âš ï¸ æš‚æ—¶ç¦ç”¨ stealth
        # if HAS_STEALTH: await stealth_async(page)
        
        try:
            # ============================================
            # ç›´æ¥è®¿é—®ç”¨æˆ·ä¸»é¡µ
            # ============================================
            print("[SYNC] ğŸš€ ç›´æ¥è®¿é—®ç›®æ ‡ç”¨æˆ·ä¸»é¡µ...")
            
            try:
                await page.goto(TOUTIAO_URL, wait_until="networkidle", timeout=45000)
                print("[SYNC] âœ“ networkidle å®Œæˆ")
            except Exception as timeout_err:
                print(f"[SYNC] âš  networkidle è¶…æ—¶ï¼Œå°è¯•é™çº§: {timeout_err}")
                try:
                    await page.goto(TOUTIAO_URL, wait_until="domcontentloaded", timeout=30000)
                    print("[SYNC] âœ“ domcontentloaded å®Œæˆ")
                except:
                    raise Exception("é¡µé¢åŠ è½½å®Œå…¨å¤±è´¥")
            
            await human_delay(4, 6)
            
            # éªŒè¯ç æ£€æŸ¥
            if await check_captcha(page, f"sync_try_{attempt}"):
                raise Exception("Captcha detected")

            articles_found = False
            links = []
            all_seen_urls = set()
            
            # ============================================
            # ç­‰å¾…æ–‡ç« å…ƒç´ å‡ºç°
            # ============================================
            article_selectors = [
                'a[href*="/article/"]',
                'a[href*="/w/"]', 
                'a[href*="/video/"]',
            ]
            
            for sel in article_selectors:
                try:
                    await page.wait_for_selector(sel, timeout=8000)
                    print(f"[SYNC] âœ“ æ£€æµ‹åˆ°æ–‡ç« å…ƒç´ : {sel}")
                    break
                except:
                    continue
            
            # ============================================
            # ğŸ”¥ æ»šåŠ¨åŠ è½½ï¼ˆæ ¹æ®æ¨¡å¼è°ƒæ•´ç­–ç•¥ï¼‰
            # ============================================
            print(f"[SYNC] å¼€å§‹æ»šåŠ¨åŠ è½½ (æœ€å¤š {max_scroll_rounds} æ¬¡)...")
            
            no_new_count = 0
            last_total = 0
            
            for scroll_round in range(max_scroll_rounds):
                # æ»šåŠ¨
                scroll_distance = random.randint(400, 700)
                await page.mouse.wheel(0, scroll_distance)
                await asyncio.sleep(random.uniform(1.5, 2.5))
                
                # å¶å°”å›æ»š
                if random.random() < 0.1:
                    await page.mouse.wheel(0, -random.randint(80, 150))
                    await asyncio.sleep(0.3)
                
                # æ¯3æ¬¡æ»šåŠ¨æå–ä¸€æ¬¡é“¾æ¥
                if (scroll_round + 1) % 3 == 0 or scroll_round == 0:
                    current_links = await page.evaluate(EXTRACT_LINKS_JS)
                    
                    # ç»Ÿè®¡æ–°å¢
                    new_urls = [l for l in current_links if l['href'] not in all_seen_urls]
                    for l in current_links:
                        all_seen_urls.add(l['href'])
                    
                    links = current_links
                    new_this_round = len(links) - last_total
                    last_total = len(links)
                    
                    # æ‰“å°è¿›åº¦ï¼ˆå…¨é‡æ¨¡å¼æ›´è¯¦ç»†ï¼‰
                    if is_full_sync:
                        print(f"[SYNC] ğŸ“Š æ»šåŠ¨ {scroll_round + 1}/{max_scroll_rounds}: "
                              f"ç´¯è®¡ {len(links)} ç¯‡ (+{new_this_round})")
                    else:
                        print(f"[SYNC] æ»šåŠ¨ {scroll_round + 1}/{max_scroll_rounds}: "
                              f"å½“å‰ {len(links)} ç¯‡")
                    
                    if links and len(links) > 0:
                        articles_found = True
                    
                    # ============================================
                    # åœæ­¢æ¡ä»¶åˆ¤æ–­
                    # ============================================
                    
                    # æ¡ä»¶1ï¼šå¢é‡æ¨¡å¼ä¸‹è·å–è¶³å¤Ÿæ–‡ç« 
                    if not is_full_sync and len(links) >= early_stop_count:
                        print(f"[SYNC] âœ“ å¢é‡æ¨¡å¼å·²è·å– {len(links)} ç¯‡ï¼Œæå‰ç»“æŸ")
                        break
                    
                    # æ¡ä»¶2ï¼šè¿ç»­æ— æ–°å†…å®¹
                    if new_this_round == 0:
                        no_new_count += 1
                        if no_new_count >= no_new_threshold:
                            if is_full_sync:
                                print(f"[SYNC] ğŸ“ å·²æ»‘åˆ°åº•éƒ¨ï¼è¿ç»­ {no_new_count} æ¬¡æ— æ–°å†…å®¹")
                            else:
                                print(f"[SYNC] è¿ç»­ {no_new_count} æ¬¡æ— æ–°å†…å®¹ï¼Œåœæ­¢")
                            break
                    else:
                        no_new_count = 0
            
            # æœ€ç»ˆç­‰å¾…
            await human_delay(2, 3)
            
            # æœ€ç»ˆæå–
            final_links = await page.evaluate(EXTRACT_LINKS_JS)
            if final_links and len(final_links) > len(links):
                links = final_links
            
            # ============================================
            # å…¨é‡æ¨¡å¼é¢å¤–åŠªåŠ›ï¼šå¦‚æœè¿˜æ²¡åˆ°åº•ï¼Œç»§ç»­æ»šåŠ¨
            # ============================================
            if is_full_sync and no_new_count < no_new_threshold:
                print("[SYNC] ğŸ”„ å…¨é‡æ¨¡å¼ï¼šç»§ç»­å°è¯•åŠ è½½æ›´å¤š...")
                extra_rounds = 20
                
                for extra in range(extra_rounds):
                    await page.mouse.wheel(0, random.randint(500, 800))
                    await asyncio.sleep(random.uniform(1.2, 2.0))
                    
                    if (extra + 1) % 5 == 0:
                        extra_links = await page.evaluate(EXTRACT_LINKS_JS)
                        new_extra = len(extra_links) - len(links)
                        
                        if extra_links:
                            links = extra_links
                        
                        print(f"[SYNC] ğŸ“Š é¢å¤–æ»šåŠ¨ {extra + 1}/{extra_rounds}: "
                              f"ç´¯è®¡ {len(links)} ç¯‡ (+{new_extra})")
                        
                        if new_extra == 0:
                            no_new_count += 1
                            if no_new_count >= 3:
                                print("[SYNC] ğŸ“ ç¡®è®¤å·²åˆ°åº•éƒ¨")
                                break
                        else:
                            no_new_count = 0
            
            print(f"\n[SYNC] æœ€ç»ˆæå–: {len(links)} ç¯‡æ–‡ç« ")
            
            # ============================================
            # é¡µé¢åˆ·æ–°é‡è¯•ï¼ˆå¦‚æœæ²¡æ‰¾åˆ°æ–‡ç« ï¼‰
            # ============================================
            if not links or len(links) == 0:
                if attempt < MAX_RETRIES:
                    print("[SYNC] æœªå‘ç°æ–‡ç« ï¼Œå°è¯•åˆ·æ–°é¡µé¢...")
                    
                    await page.screenshot(path=DEBUG_DIR / f"before_refresh_attempt_{attempt}.png")
                    
                    for refresh_attempt in range(2):
                        print(f"[SYNC] ç¬¬ {refresh_attempt + 1} æ¬¡åˆ·æ–°...")
                        await page.reload(wait_until="networkidle", timeout=30000)
                        await human_delay(5, 7)
                        
                        for i in range(10):
                            await page.mouse.wheel(0, random.randint(400, 600))
                            await asyncio.sleep(random.uniform(0.8, 1.2))
                        
                        await human_delay(3, 5)
                        
                        links = await page.evaluate(EXTRACT_LINKS_JS)
                        if links and len(links) > 0:
                            articles_found = True
                            print(f"[SYNC] âœ“ åˆ·æ–°åå‘ç° {len(links)} ç¯‡æ–‡ç« ")
                            break
            else:
                articles_found = True
            
            # ============================================
            # ç»“æœåˆ¤æ–­
            # ============================================
            if articles_found and links and len(links) > 0:
                # ========== æˆåŠŸ ==========
                mode_str = "å…¨é‡" if is_full_sync else "å¢é‡"
                print(f"\n[SYNC] âœ… {mode_str}åŒæ­¥æˆåŠŸ! ç¬¬ {attempt} æ¬¡å°è¯•ï¼Œå…± {len(links)} ç¯‡æ–‡ç« ")
                
                # æ‰“å°ç»Ÿè®¡
                new_articles = [l for l in links if l['href'] not in db.data.get("articles", {})]
                print(f"[SYNC] ğŸ“ˆ å…¶ä¸­æ–°æ–‡ç« : {len(new_articles)} ç¯‡")
                
                # æ‰“å°æ ·æœ¬
                print("[SYNC] æ–‡ç« æ ·æœ¬:")
                for i, link in enumerate(links[:5], 1):
                    print(f"       {i}. [{link.get('type', '?')}] {link['text'][:40]}...")
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                db.add_articles(links)
                db.mark_synced()
                
                # ä¿å­˜æˆåŠŸæˆªå›¾
                try:
                    await page.screenshot(path=DEBUG_DIR / "sync_success_latest.png")
                    print("[SYNC] âœ“ å·²ä¿å­˜æˆåŠŸæˆªå›¾")
                except: pass
                
                # æ¸…ç†æ—§é”™è¯¯æ–‡ä»¶
                print("[SYNC] æ¸…ç†æ—§çš„è°ƒè¯•/é”™è¯¯æ–‡ä»¶...")
                cleanup_patterns = [
                    "error_sync_*.png",
                    "debug_sync_fail_*.png",
                    "before_refresh_*.png",
                    "sync_source_*.html",
                    "captcha_sync_*.png",
                    "sync_source_final_fail.html",
                ]
                
                cleaned_count = 0
                try:
                    for pattern in cleanup_patterns:
                        for file_path in DEBUG_DIR.glob(pattern):
                            try:
                                file_path.unlink(missing_ok=True)
                                cleaned_count += 1
                            except:
                                pass
                    if cleaned_count > 0:
                        print(f"[SYNC] âœ“ å·²æ¸…ç† {cleaned_count} ä¸ªæ—§æ–‡ä»¶")
                except Exception as clean_err:
                    print(f"[WARN] æ¸…ç†æ–‡ä»¶æ—¶å‡ºé”™: {clean_err}")
                
                await page.close()
                return
                
            else:
                # ========== å¤±è´¥ ==========
                print(f"[WARN] ç¬¬ {attempt} æ¬¡å°è¯•æœªèƒ½æå–åˆ°æ–‡ç« ")
                
                try:
                    await page.screenshot(path=DEBUG_DIR / f"debug_sync_fail_attempt_{attempt}.png")
                except: pass
                
                try:
                    content = await page.content()
                    (DEBUG_DIR / f"sync_source_attempt_{attempt}.html").write_text(
                        content, encoding="utf-8"
                    )
                except: pass
                
                if attempt < MAX_RETRIES:
                    raise Exception("No links extracted")

        except Exception as e:
            print(f"[SYNC] âŒ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {e}")
            
            try:
                if not page.is_closed():
                    await page.screenshot(path=DEBUG_DIR / f"error_sync_attempt_{attempt}.png")
            except: pass
            
            if attempt == MAX_RETRIES:
                print("[FATAL] âŒ å…¨é‡åŒæ­¥ä»»åŠ¡æœ€ç»ˆå¤±è´¥")
                try:
                    if not page.is_closed():
                        content = await page.content()
                        (DEBUG_DIR / "sync_source_final_fail.html").write_text(
                            content, encoding="utf-8"
                        )
                except: pass
            else:
                wait_time = random.randint(5, 10)
                print(f"[WAIT] ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)
        
        finally:
            try:
                if not page.is_closed():
                    await page.close()
            except: pass
    
    print("[SYNC] âŒ å…¨é‡åŒæ­¥ä»»åŠ¡å®Œå…¨å¤±è´¥")


async def read_article_task(context: BrowserContext, article: dict, db: ArticleDB):
    """
    å•ç¯‡é˜…è¯»ä»»åŠ¡ - å®Œæ•´å¢å¼ºç‰ˆ
    åŒ…å«ï¼šåŒé‡æˆªå›¾éªŒè¯ã€æ·±åº¦æ‹ŸäººåŒ–æ“ä½œã€æ™ºèƒ½æ–‡ä»¶æ¸…ç†
    """
    url = article['url']
    
    # ä» URL ä¸­æå–ç®€å•çš„ ID ç”¨äºæ–‡ä»¶åï¼ˆé˜²æ­¢æ–‡ä»¶åè¿‡é•¿ï¼‰
    try:
        # å°è¯•æå–æœ€åä¸€æ®µæ•°å­—æˆ–å­—ç¬¦ä½œä¸ºID
        article_id = url.split('/')[-1].split('?')[0][-12:]
    except:
        article_id = "unknown"

    title_preview = article['title'][:30]
    print(f"--- [READ] æ­£åœ¨æ‰“å¼€: {title_preview}... ---")
    
    page = await context.new_page()
    
    # ç”Ÿæˆæœ¬æ¬¡ä»»åŠ¡çš„æ—¶é—´æˆ³å­—ç¬¦ä¸² (æ—¶åˆ†ç§’)
    timestamp_str = datetime.now().strftime("%H%M%S")

    # âš ï¸ æš‚æ—¶ç¦ç”¨ stealthï¼Œå› ä¸ºéƒ¨åˆ†ç¯å¢ƒä¸‹ä¼šå¯¼è‡´æ£€æµ‹åŠ é‡ï¼Œå¯è§†æƒ…å†µå¼€å¯
    # if HAS_STEALTH: await stealth_async(page)

    try:
        # ============================================
        # 1. é¡µé¢è®¿é—®ä¸é¦–å±éªŒè¯
        # ============================================
        # domcontentloaded æ¯” networkidle æ›´å¿«ï¼Œé€‚åˆæœ‰å¹¿å‘Šæµçš„é¡µé¢
        await page.goto(url, wait_until="domcontentloaded", timeout=45000)
        
        # å¼ºåˆ¶ç­‰å¾… 3 ç§’ï¼Œè®©å›¾ç‰‡å’Œ JS æ¸²æŸ“å‡ºæ¥ï¼Œç¡®ä¿æˆªå›¾ä¸æ˜¯ç™½çš„
        await asyncio.sleep(3)

        # --------------------------------------------
        # ğŸ”¥ã€å…³é”®ã€‘é¦–å±æˆªå›¾ (START) - è¯æ˜æ–‡ç« åŠ è½½å‡ºæ¥äº†
        # --------------------------------------------
        try:
            start_ss_name = f"read_{timestamp_str}_{article_id}_START.png"
            # full_page=False åªæˆªå½“å‰è§†å£ï¼Œé¿å…å¤ªé•¿å¯¼è‡´æŠ¥é”™
            await page.screenshot(path=DEBUG_DIR / start_ss_name, full_page=False)
            print(f"[READ] ğŸ“¸ é¦–å±å†…å®¹å·²ä¿å­˜: {start_ss_name}")
        except Exception as e:
            print(f"[WARN] é¦–å±æˆªå›¾å¤±è´¥: {e}")

        # --------------------------------------------
        # å¼‚å¸¸æ£€æµ‹
        # --------------------------------------------
        # A. éªŒè¯ç æ£€æŸ¥ (è°ƒç”¨å¤–éƒ¨å®šä¹‰çš„ check_captcha)
        if await check_captcha(page, "read_start"):
            return

        # B. è·å–é¡µé¢å…³é”®ä¿¡æ¯
        page_content = await page.evaluate("document.body.innerText")
        page_title = await page.title()
        
        # C. 404/å¤±æ•ˆåˆ¤æ–­
        invalid_keywords = ["404", "é¡µé¢ä¸å­˜åœ¨", "æ–‡ç« å·²åˆ é™¤", "å‚æ•°é”™è¯¯", "è®¿é—®å—é™"]
        if any(k in page_title for k in invalid_keywords):
            print("[READ] âŒ æ–‡ç« å·²å¤±æ•ˆï¼Œæ ‡è®° invalidã€‚")
            db.mark_invalid(url)
            return

        # =========================================================
        # 2. æ™ºèƒ½é˜…è¯»æ—¶é•¿è®¡ç®— (æ‹ŸäººåŒ–æ ¸å¿ƒ)
        # =========================================================
        
        # A. ç»Ÿè®¡å­—æ•°
        word_count = len(page_content)
        
        # B. ç»Ÿè®¡å›¾ç‰‡æ•°é‡ (JS æ³¨å…¥)
        img_count = await page.evaluate("""
            () => {
                // æŸ¥æ‰¾å¤´æ¡å¸¸è§çš„æ–‡ç« æ­£æ–‡åŒºåŸŸå†…çš„å›¾ç‰‡
                const imgs = document.querySelectorAll('article img, .tt-input__content img, .article-content img, .pgc-img img');
                return imgs.length;
            }
        """)

        # C. è®¡ç®—åŸºå‡†æ—¶é•¿
        # å‡è®¾ï¼šäººçœ¼æ¯ç§’æ‰«è§† 25 ä¸ªå­—ï¼Œæ¯å¼ å›¾çœ‹ 5 ç§’
        text_time = word_count / 25.0  
        img_time = img_count * 5.0
        base_time = text_time + img_time
        
        # å…œåº•ï¼šå¦‚æœæ²¡æå–åˆ°å†…å®¹ï¼Œç»™ä¸€ä¸ªéšæœºåŸºç¡€å€¼
        if base_time < 10:
            base_time = random.randint(20, 40)
        
        # D. å¢åŠ éšæœºæ‰°åŠ¨ (æ­£æ€åˆ†å¸ƒ)
        variation = random.gauss(1.0, 0.2) # å‡å€¼1.0ï¼Œæ ‡å‡†å·®0.2
        thinking_time = random.uniform(5, 15) # é¢å¤–çš„æ€è€ƒ/å‘å‘†æ—¶é—´
        
        # è®¡ç®—æ€»æ—¶é•¿
        calc_seconds = (base_time * variation) + thinking_time
        
        # E. ä¸¥æ ¼æˆªæ–­èŒƒå›´ (æœ€å°‘è¯» 30sï¼Œæœ€å¤šè¯» 180s)
        read_seconds = max(30.0, calc_seconds)
        read_seconds = min(180.0, read_seconds)
        
        print(f"[READ] ç»Ÿè®¡: {word_count}å­— | {img_count}å›¾ | ç®—æ³•è®¡ç®—:{calc_seconds:.1f}s")
        print(f"[READ] >> â±ï¸ æœ€ç»ˆè®¡åˆ’åœç•™: {read_seconds:.1f}ç§’")
        
        # =========================================================
        # 3. æ‹ŸäººåŒ–äº¤äº’å¾ªç¯ (é‡ä¸­ä¹‹é‡)
        # =========================================================
        start_read = time.time()
        scroll_count = 0
        
        # åœ¨è§„å®šæ—¶é—´å†…å¾ªç¯æ“ä½œ
        while (time.time() - start_read) < read_seconds:
            
            # --- åŠ¨ä½œ 1: éšæœºä¸‹æ»‘ (è°ƒç”¨å¤–éƒ¨ human_scroll) ---
            # æ¯æ¬¡åªæ»‘ä¸€ç‚¹ç‚¹ï¼Œæ¨¡æ‹Ÿè¾¹çœ‹è¾¹æ»‘
            await human_scroll(page, max_scrolls=1)
            scroll_count += 1
            
            # --- åŠ¨ä½œ 2: è´å¡å°”æ›²çº¿é¼ æ ‡ç§»åŠ¨ (è°ƒç”¨å¤–éƒ¨ human_mouse_move) ---
            # 30% çš„æ¦‚ç‡ç§»åŠ¨é¼ æ ‡ï¼Œæ¨¡æ‹Ÿäººåœ¨çœ‹æŸäº›æ®µè½æ—¶é¼ æ ‡æ— æ„è¯†æ™ƒåŠ¨
            if random.random() < 0.3:
                # éšæœºç”Ÿæˆç›®æ ‡ç‚¹
                rand_x = random.randint(200, 1000)
                rand_y = random.randint(300, 800)
                await human_mouse_move(page, rand_x, rand_y)
            
            # --- åŠ¨ä½œ 3: æ¨¡æ‹Ÿæ–‡æœ¬é€‰ä¸­ (æä½æ¦‚ç‡) ---
            # 10% çš„æ¦‚ç‡ç‚¹å‡»ä¸€ä¸‹æ®µè½æ–‡å­—ï¼Œå¾ˆå¤šäººé˜…è¯»æ—¶æœ‰è¿™ä¸ªä¹ æƒ¯
            if random.random() < 0.1:
                try:
                    # å°è¯•ç‚¹å‡»ä¸€ä¸ª p æ ‡ç­¾
                    await page.click("p", timeout=200, force=True) 
                except: 
                    pass
            
            # --- åŠ¨ä½œ 4: éšæœºåœé¡¿ (æ¨¡æ‹Ÿæ€è€ƒ/è¢«æ‰“æ–­) ---
            # 5% çš„æ¦‚ç‡åœé¡¿è¾ƒé•¿æ—¶é—´ (2-5ç§’)
            if random.random() < 0.05:
                # print("[ACT] æ¨¡æ‹Ÿæ€è€ƒæš‚åœ...")
                await asyncio.sleep(random.uniform(2.0, 5.0))
            
            # --- å¾ªç¯é—´éš” ---
            # æ¯æ¬¡æ“ä½œå®Œï¼Œç­‰å¾…ä¸€å°ä¼šå„¿ï¼Œé¿å…æ“ä½œå¤ªå¯†é›†
            await asyncio.sleep(random.uniform(0.8, 2.0))

        # ============================================
        # 4. ç»“æŸåŠ¨ä½œä¸éªŒè¯
        # ============================================
        
        # å¿…é¡»åŠ¨ä½œï¼šæ»‘åŠ¨åˆ°é¡µé¢æœ€åº•éƒ¨ (è§¦å‘"å·²é˜…è¯»"åŸ‹ç‚¹)
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await human_delay(1.5, 3.0)
        
        print(f"[READ] âœ… é˜…è¯»æ—¶é—´è¾¾æ ‡ (æ»šåŠ¨{scroll_count}æ¬¡)")
        
        # --------------------------------------------
        # ğŸ”¥ã€å…³é”®ã€‘å®Œè¯»æˆªå›¾ (END) - è¯æ˜è¯»åˆ°åº•äº†
        # --------------------------------------------
        end_ss_name = f"read_{timestamp_str}_{article_id}_END.png"
        try:
            await page.screenshot(path=DEBUG_DIR / end_ss_name)
            print(f"[READ] ğŸ“¸ å®Œè¯»åº•éƒ¨å·²ä¿å­˜: {end_ss_name}")
        except Exception as ss_err:
            print(f"[READ] âš  æˆªå›¾å¤±è´¥: {ss_err}")
        
        # è®°å½•é˜…è¯»çŠ¶æ€åˆ°æ•°æ®åº“
        db.record_read(url)

        # ============================================
        # 5. æ–‡ä»¶æ¸…ç†é€»è¾‘ (é˜²æ­¢ç£ç›˜å æ»¡)
        # ============================================
        try:
            # æ¸…ç† START æˆªå›¾ï¼šæŒ‰æ—¶é—´å€’åºæ’ï¼Œåªä¿ç•™æœ€æ–°çš„ 5 å¼ 
            start_files = sorted(DEBUG_DIR.glob("*_START.png"), key=lambda x: x.stat().st_mtime, reverse=True)
            for old_file in start_files[5:]:
                old_file.unlink(missing_ok=True)

            # æ¸…ç† END æˆªå›¾ï¼šæŒ‰æ—¶é—´å€’åºæ’ï¼Œåªä¿ç•™æœ€æ–°çš„ 5 å¼ 
            end_files = sorted(DEBUG_DIR.glob("*_END.png"), key=lambda x: x.stat().st_mtime, reverse=True)
            for old_file in end_files[5:]:
                old_file.unlink(missing_ok=True)
            
            # æ¸…ç†é”™è¯¯æˆªå›¾ï¼šåªä¿ç•™æœ€æ–°çš„ 3 å¼ 
            error_files = sorted(DEBUG_DIR.glob("error_read_*.png"), key=lambda x: x.stat().st_mtime, reverse=True)
            for old_file in error_files[3:]:
                old_file.unlink(missing_ok=True)

        except Exception as clean_err:
            print(f"[READ] âš  æ¸…ç†æ—§æˆªå›¾å¤±è´¥: {clean_err}")

    except Exception as e:
        print(f"[READ] âŒ å¼‚å¸¸ä¸­æ–­: {e}")
        # å‘ç”Ÿå¼‚å¸¸æ—¶çš„ç´§æ€¥æˆªå›¾
        try:
            err_name = f"error_read_{timestamp_str}.png"
            await page.screenshot(path=DEBUG_DIR / err_name)
            print(f"[READ] å·²ä¿å­˜é”™è¯¯ç°åœº: {err_name}")
        except:
            pass
    
    finally:
        # ç¡®ä¿é¡µé¢å…³é—­ï¼Œé‡Šæ”¾å†…å­˜
        try:
            if not page.is_closed():
                await page.close()
        except:
            pass
            


# ================= ä¸»ç¨‹åºå…¥å£ =================

async def main():
    # 1. å‡†å¤‡å·¥ä½œ
    db = ArticleDB(DB_FILE)
    
    # éšæœºé€‰æ‹©è§†çª—
    vp = random.choice(VIEWPORTS)
    # è·å–éšæœº UA
    ua = get_pc_user_agent()
    
    print(f"[INIT] å¯åŠ¨çˆ¬è™«ä»»åŠ¡")
    print(f"[INIT] UA: {ua[:50]}...")
    print(f"[INIT] Viewport: {vp['width']}x{vp['height']}")

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-infobars",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                f"--window-size={vp['width']},{vp['height']}"
            ]
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = await browser.new_context(
            user_agent=ua,
            viewport=vp,
            locale="zh-CN",
            timezone_id="Asia/Shanghai",
            device_scale_factor=1,
            has_touch=False,
            is_mobile=False,
            java_script_enabled=True
        )

        # æ³¨å…¥ webdriver ç§»é™¤è„šæœ¬
        await context.add_init_script(
            "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        )

        # ============================================
        # ğŸ”¥ ç»Ÿä¸€çƒ­èº«ï¼šåœ¨æ‰€æœ‰ä»»åŠ¡ä¹‹å‰ï¼Œç”¨ç‹¬ç«‹ Page
        # ============================================
        print("[WARMUP] æ‰§è¡Œä¸€æ¬¡æ€§çƒ­èº«...")
        warmup_page = None
        try:
            warmup_page = await context.new_page()
            await warmup_page.goto(
                "https://www.toutiao.com/", 
                wait_until="domcontentloaded",  # ä¸ç”¨ networkidle
                timeout=30000
            )
            await human_delay(2, 4)
            
            # ç®€å•äº¤äº’
            await human_mouse_move(warmup_page, 500, 400)
            await warmup_page.mouse.wheel(0, random.randint(200, 400))
            await human_delay(1, 2)
            
            print("[WARMUP] âœ“ çƒ­èº«å®Œæˆ")
        except Exception as e:
            print(f"[WARMUP] âš  çƒ­èº«å¤±è´¥(å¯å¿½ç•¥): {e}")
        finally:
            if warmup_page:
                try:
                    await warmup_page.close()
                except:
                    pass
        
        # çƒ­èº«åçŸ­æš‚ç­‰å¾…
        await asyncio.sleep(random.uniform(1, 2))

        # ============================================
        # æ­¥éª¤ 1: æ£€æŸ¥æ˜¯å¦éœ€è¦å…¨é‡åŒæ­¥
        # ============================================
        if db.needs_sync() or not db.data.get("articles"):
            print("\n[TASK] å¼€å§‹åŒæ­¥ä»»åŠ¡...")
            await sync_task(context, db)
        else:
            print("[INIT] ä»Šæ—¥å·²æ‰§è¡Œè¿‡åŒæ­¥ï¼Œè·³è¿‡åˆ—è¡¨æŠ“å–ã€‚")

        # ============================================
        # æ­¥éª¤ 2: è·å–ä»Šæ—¥é˜…è¯»ç›®æ ‡
        # ============================================
        targets = db.get_weighted_candidates()
        
        if not targets:
            print("[DONE] æš‚æ— å¾…è¯»æ–‡ç«  (å¯èƒ½å·²å…¨éƒ¨è¯»å®Œæˆ–æ— æ–°å†…å®¹)ã€‚")
            await browser.close()
            return

        print(f"\n[TASK] ä»Šæ—¥é˜…è¯»è®¡åˆ’: {len(targets)} ç¯‡æ–‡ç« ")

        # ============================================
        # æ­¥éª¤ 3: å¾ªç¯é˜…è¯»
        # æ³¨æ„ï¼šçƒ­èº«å·²å®Œæˆï¼Œæ¯ç¯‡æ–‡ç« ç›´æ¥è®¿é—®
        # ============================================
        for i, article in enumerate(targets, 1):
            print(f"\n{'='*50}")
            print(f">>> è¿›åº¦ [{i}/{len(targets)}]")
            print(f"{'='*50}")
            
            await read_article_task(context, article, db)
            
            # ç¯‡é—´å†·å´æ—¶é—´
            if i < len(targets):
                wait_time = random.randint(8, 15)
                print(f"[COOL] ä¼‘æ¯ {wait_time} ç§’...")
                await asyncio.sleep(wait_time)

        # ============================================
        # å®Œæˆ
        # ============================================
        await browser.close()
        print("\n" + "="*50)
        print("[DONE] âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        print("="*50)


if __name__ == "__main__":
    asyncio.run(main())


